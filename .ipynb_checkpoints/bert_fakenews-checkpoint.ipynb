{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 版本： 1.3.1+cu92\n",
      "字典大小： 21128\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "#from IPython.display import clear_output\n",
    "\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-chinese\"  # 指定繁簡中文 BERT-BASE 預訓練模型\n",
    "\n",
    "# 取得此預訓練模型所使用的 tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "\n",
    "#clear_output()\n",
    "print(\"PyTorch 版本：\", torch.__version__)\n",
    "#====================================================\n",
    "vocab = tokenizer.vocab\n",
    "print(\"字典大小：\", len(vocab))\n",
    "#====================================================\n",
    "text = \"[CLS] 等到潮水 [MASK] 了，就知道誰沒穿褲子。\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練樣本數： 2657\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_a</th>\n",
       "      <th>text_b</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>苏有朋要结婚了，但网友觉得他还是和林心如比较合适</td>\n",
       "      <td>好闺蜜结婚给不婚族的秦岚扔花球，倒霉的秦岚掉水里笑哭苏有朋！</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>爆料李小璐要成前妻了贾乃亮模仿王宝强一步到位、快刀斩乱麻！</td>\n",
       "      <td>李小璐要变前妻了？贾乃亮可能效仿王宝强当机立断，快刀斩乱麻！</td>\n",
       "      <td>agreed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>为彩礼，母亲把女儿嫁给陌生男子，十年后再见面，母亲湿了眼眶</td>\n",
       "      <td>阿姨，不要彩礼是觉得你家穷，给你台阶下，不要以为我嫁不出去！</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>猪油是个宝，一勺猪油等于十副药，先备起来再说</td>\n",
       "      <td>传承千百的猪油为何变得人人唯恐避之不及？揭开猪油的四大谣言！</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>剖析：香椿，为什么会致癌？</td>\n",
       "      <td>香椿含亚硝酸盐多吃会致癌？测完发现是谣言</td>\n",
       "      <td>disagreed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          text_a                          text_b      label\n",
       "0       苏有朋要结婚了，但网友觉得他还是和林心如比较合适  好闺蜜结婚给不婚族的秦岚扔花球，倒霉的秦岚掉水里笑哭苏有朋！  unrelated\n",
       "1  爆料李小璐要成前妻了贾乃亮模仿王宝强一步到位、快刀斩乱麻！  李小璐要变前妻了？贾乃亮可能效仿王宝强当机立断，快刀斩乱麻！     agreed\n",
       "2  为彩礼，母亲把女儿嫁给陌生男子，十年后再见面，母亲湿了眼眶  阿姨，不要彩礼是觉得你家穷，给你台阶下，不要以为我嫁不出去！  unrelated\n",
       "3         猪油是个宝，一勺猪油等于十副药，先备起来再说  传承千百的猪油为何变得人人唯恐避之不及？揭开猪油的四大谣言！  unrelated\n",
       "4                  剖析：香椿，为什么会致癌？            香椿含亚硝酸盐多吃会致癌？测完发现是谣言  disagreed"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# 簡單的數據清理，去除空白標題的 examples\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "empty_title = ((df_train['title2_zh'].isnull()) \\\n",
    "               | (df_train['title1_zh'].isnull()) \\\n",
    "               | (df_train['title2_zh'] == '') \\\n",
    "               | (df_train['title2_zh'] == '0'))\n",
    "df_train = df_train[~empty_title]\n",
    "\n",
    "# 剔除過長的樣本以避免 BERT 無法將整個輸入序列放入記憶體不多的 GPU\n",
    "MAX_LENGTH = 30\n",
    "df_train = df_train[~(df_train.title1_zh.apply(lambda x : len(x)) > MAX_LENGTH)]\n",
    "df_train = df_train[~(df_train.title2_zh.apply(lambda x : len(x)) > MAX_LENGTH)]\n",
    "\n",
    "# 只用 1% 訓練數據看看 BERT 對少量標註數據有多少幫助\n",
    "SAMPLE_FRAC = 0.01\n",
    "df_train = df_train.sample(frac=SAMPLE_FRAC, random_state=9527)\n",
    "\n",
    "# 去除不必要的欄位並重新命名兩標題的欄位名\n",
    "df_train = df_train.reset_index()\n",
    "df_train = df_train.loc[:, ['title1_zh', 'title2_zh', 'label']]\n",
    "df_train.columns = ['text_a', 'text_b', 'label']\n",
    "#df_train = df_train.iloc[0:1]\n",
    "#df_train.loc[0][1] = '儘管中國宣稱2019 新型冠狀病毒（COVID-19）疫情在境內受到控制，但中國官方和CBA聯盟不同調，複賽仍遙遙無期，加上中國又頒布外國人禁令，林書豪等外援提早返中恐都是白跑一趟。從疫情爆發後CBA自1月開始停賽，日前宣布將在4月15日重啟，還曾傳出外援包含林書豪等人若不歸隊，將面臨終身禁賽等處分，逼迫外援們提早返中進行14天隔離檢疫，以利備戰複賽。林書豪在3月19日於社群媒體PO出照片，證明他已經返抵中國，就在一切蓄勢待發下，中國國家體育總局卻在3月31日公告暫緩馬拉松等體育賽事，理由為貫徹「外防輸入、內防反彈」控制疫情。先前3月28日，中國官方宣布旅遊禁令，暫停外國人入境，多隊外援和外籍教頭可能無法如期出賽，CBA本來和各隊取得初步共識，以全本土開戰，但如今當局下令不准打，什麼方案都是白談了，外援提早入境恐怕也都是白跑一趟，目前何時複賽仍未明朗。\t儘管中國宣稱2019 新型冠狀病毒（COVID-19）疫情在境內受到控制，但中國官方和CBA聯盟不同調，複賽仍遙遙無期，加上中國又頒布外國人禁令，林書豪等外援提早返中恐都是白跑一趟。從疫情爆發後CBA自1月開始停賽，日前宣布將在4月15日重啟，還曾傳出外援包含林書豪等人若不歸隊，將面臨終身禁賽等處分，逼迫外援們提早返中進行14天隔離檢疫，以利備戰複賽。林書豪在3月19日於社群媒體PO出照片，證明他已經返抵中國，就在一切蓄勢待發下，中國國家體育總局卻在3月31日公告暫緩馬拉松等體育賽事，理由為貫徹「外防輸入、內防反彈」控制疫情。先前3月28日，中國官方宣布旅遊禁令，暫停外國人入境，多隊外援和外籍教頭可能無法如期出賽，CBA本來和各隊取得初步共識，以全本土開戰，但如今當局下令不准打，什麼方案都是白談了，外援提早入境恐怕也都是白跑一趟，目前何時複賽仍未明朗。'\n",
    "# idempotence, 將處理結果另存成 tsv 供 PyTorch 使用\n",
    "df_train.to_csv(\"train.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "print(\"訓練樣本數：\", len(df_train))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "預測樣本數： 80126\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_a</th>\n",
       "      <th>text_b</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大</td>\n",
       "      <td>辟谣！里昂官方否认费基尔加盟利物浦，难道是价格没谈拢？</td>\n",
       "      <td>321187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>10大最让美国人相信的荒诞谣言，如蜥蜴人掌控着美国</td>\n",
       "      <td>321190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>萨达姆此项计划没有此国破坏的话，美国还会对伊拉克发动战争吗</td>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>321189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>被绞刑处死的萨达姆是替身？他的此男人举动击破替身谣言！</td>\n",
       "      <td>321193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>中国川贝枇杷膏在美国受到热捧？纯属谣言！</td>\n",
       "      <td>321191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            text_a                       text_b      Id\n",
       "0  萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大  辟谣！里昂官方否认费基尔加盟利物浦，难道是价格没谈拢？  321187\n",
       "1              萨达姆被捕后告诫美国的一句话，发人深思    10大最让美国人相信的荒诞谣言，如蜥蜴人掌控着美国  321190\n",
       "2    萨达姆此项计划没有此国破坏的话，美国还会对伊拉克发动战争吗          萨达姆被捕后告诫美国的一句话，发人深思  321189\n",
       "3              萨达姆被捕后告诫美国的一句话，发人深思  被绞刑处死的萨达姆是替身？他的此男人举动击破替身谣言！  321193\n",
       "4              萨达姆被捕后告诫美国的一句话，发人深思         中国川贝枇杷膏在美国受到热捧？纯属谣言！  321191"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"test.csv\")\n",
    "df_test = df_test.loc[:, [\"title1_zh\", \"title2_zh\", \"id\"]]\n",
    "df_test.columns = [\"text_a\", \"text_b\", \"Id\"]\n",
    "df_test.to_csv(\"test.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "print(\"預測樣本數：\", len(df_test))\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pysnooper\n",
    "    \n",
    "class FakeNewsDataset(Dataset):\n",
    "    # 讀取前處理後的 tsv 檔並初始化一些參數\n",
    "    def __init__(self, mode, tokenizer):\n",
    "        assert mode in [\"train\", \"test\"]  # 一般訓練你會需要 dev set\n",
    "        self.mode = mode\n",
    "        # 大數據你會需要用 iterator=True\n",
    "        self.df = pd.read_csv(mode + \".tsv\", sep=\"\\t\").fillna(\"\")\n",
    "        self.len = len(self.df)\n",
    "        self.label_map = {'agreed': 0, 'disagreed': 1, 'unrelated': 2}\n",
    "        self.tokenizer = tokenizer  # 我們將使用 BERT tokenizer\n",
    "    @pysnooper.snoop()\n",
    "    # 定義回傳一筆訓練 / 測試數據的函式\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == \"test\":\n",
    "            text_a, text_b = self.df.iloc[idx, :2].values\n",
    "            label_tensor = None\n",
    "        else:\n",
    "            text_a, text_b, label = self.df.iloc[idx, :].values\n",
    "            # 將 label 文字也轉換成索引方便轉換成 tensor\n",
    "            label_id = self.label_map[label]\n",
    "            label_tensor = torch.tensor(label_id)\n",
    "            \n",
    "        # 建立第一個句子的 BERT tokens 並加入分隔符號 [SEP]\n",
    "        word_pieces = [\"[CLS]\"]\n",
    "        tokens_a = self.tokenizer.tokenize(text_a)\n",
    "        word_pieces += tokens_a + [\"[SEP]\"]\n",
    "        len_a = len(word_pieces)\n",
    "        \n",
    "        # 第二個句子的 BERT tokens\n",
    "        tokens_b = self.tokenizer.tokenize(text_b)\n",
    "        word_pieces += tokens_b + [\"[SEP]\"]\n",
    "        len_b = len(word_pieces) - len_a\n",
    "        \n",
    "        # 將整個 token 序列轉換成索引序列\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "        \n",
    "        # 將第一句包含 [SEP] 的 token 位置設為 0，其他為 1 表示第二句\n",
    "        segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
    "                                        dtype=torch.long)\n",
    "        \n",
    "        return (tokens_tensor, segments_tensor, label_tensor)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    \n",
    "# 初始化一個專門讀取訓練樣本的 Dataset，使用中文 BERT 斷詞\n",
    "trainset = FakeNewsDataset(\"train\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[原始文本]\n",
      "句子 1：苏有朋要结婚了，但网友觉得他还是和林心如比较合适\n",
      "句子 2：好闺蜜结婚给不婚族的秦岚扔花球，倒霉的秦岚掉水里笑哭苏有朋！\n",
      "分類  ：unrelated\n",
      "\n",
      "--------------------\n",
      "\n",
      "[Dataset 回傳的 tensors]\n",
      "tokens_tensor  ：tensor([ 101, 5722, 3300, 3301, 6206, 5310, 2042,  749, 8024,  852, 5381, 1351,\n",
      "        6230, 2533,  800, 6820, 3221, 1469, 3360, 2552, 1963, 3683, 6772, 1394,\n",
      "        6844,  102, 1962, 7318, 6057, 5310, 2042, 5314,  679, 2042, 3184, 4638,\n",
      "        4912, 2269, 2803, 5709, 4413, 8024,  948, 7450, 4638, 4912, 2269, 2957,\n",
      "        3717, 7027, 5010, 1526, 5722, 3300, 3301, 8013,  102])\n",
      "\n",
      "segments_tensor：tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "\n",
      "label_tensor   ：2\n",
      "\n",
      "--------------------\n",
      "\n",
      "[還原 tokens_tensors]\n",
      "[CLS]苏有朋要结婚了，但网友觉得他还是和林心如比较合适[SEP]好闺蜜结婚给不婚族的秦岚扔花球，倒霉的秦岚掉水里笑哭苏有朋！[SEP]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Source path:... <ipython-input-80-ed1903d6133d>\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x0000019E7AD14D08>\n",
      "Starting var:.. idx = 0\n",
      "13:52:07.250323 call        16     def __getitem__(self, idx):\n",
      "13:52:07.250323 line        17         if self.mode == \"test\":\n",
      "13:52:07.250323 line        21             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '苏有朋要结婚了，但网友觉得他还是和林心如比较合适'\n",
      "New var:....... text_b = '好闺蜜结婚给不婚族的秦岚扔花球，倒霉的秦岚掉水里笑哭苏有朋！'\n",
      "New var:....... label = 'unrelated'\n",
      "13:52:07.251321 line        23             label_id = self.label_map[label]\n",
      "New var:....... label_id = 2\n",
      "13:52:07.251321 line        24             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(2)\n",
      "13:52:07.251321 line        27         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "13:52:07.251321 line        28         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['苏', '有', '朋', '要', '结', '婚', '了', '，', '但', '网...'还', '是', '和', '林', '心', '如', '比', '较', '合', '适']\n",
      "13:52:07.252324 line        29         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '苏', '有', '朋', '要', '结', '婚', '了', '，'... '和', '林', '心', '如', '比', '较', '合', '适', '[SEP]']\n",
      "13:52:07.252324 line        30         len_a = len(word_pieces)\n",
      "New var:....... len_a = 26\n",
      "13:52:07.253315 line        33         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['好', '闺', '蜜', '结', '婚', '给', '不', '婚', '族', '的...'岚', '掉', '水', '里', '笑', '哭', '苏', '有', '朋', '！']\n",
      "13:52:07.255310 line        34         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '苏', '有', '朋', '要', '结', '婚', '了', '，'... '水', '里', '笑', '哭', '苏', '有', '朋', '！', '[SEP]']\n",
      "13:52:07.256307 line        35         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 31\n",
      "13:52:07.256307 line        38         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 5722, 3300, 3301, 6206, 5310, 2042, 749, 8...7, 7027, 5010, 1526, 5722, 3300, 3301, 8013, 102]\n",
      "13:52:07.257304 line        39         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5722, 3300, 3301, 6206, 5310, 2042... 7027, 5010, 1526, 5722, 3300, 3301, 8013,  102])\n",
      "13:52:07.257304 line        42         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "13:52:07.258302 line        43                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "13:52:07.258302 line        45         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "13:52:07.259299 return      45         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 5722, 3300, 3301, 6206, 5310, 204...1,        1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.010971\n"
     ]
    }
   ],
   "source": [
    "# 選擇第一個樣本\n",
    "sample_idx = 0\n",
    "\n",
    "# 將原始文本拿出做比較\n",
    "text_a, text_b, label = trainset.df.iloc[sample_idx].values\n",
    "\n",
    "# 利用剛剛建立的 Dataset 取出轉換後的 id tensors\n",
    "tokens_tensor, segments_tensor, label_tensor = trainset[sample_idx]\n",
    "\n",
    "# 將 tokens_tensor 還原成文本\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokens_tensor.tolist())\n",
    "combined_text = \"\".join(tokens)\n",
    "\n",
    "# 渲染前後差異，毫無反應就是個 print。可以直接看輸出結果\n",
    "print(f\"\"\"[原始文本]\n",
    "句子 1：{text_a}\n",
    "句子 2：{text_b}\n",
    "分類  ：{label}\n",
    "\n",
    "--------------------\n",
    "\n",
    "[Dataset 回傳的 tensors]\n",
    "tokens_tensor  ：{tokens_tensor}\n",
    "\n",
    "segments_tensor：{segments_tensor}\n",
    "\n",
    "label_tensor   ：{label_tensor}\n",
    "\n",
    "--------------------\n",
    "\n",
    "[還原 tokens_tensors]\n",
    "{combined_text}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# 這個函式的輸入 `samples` 是一個 list，裡頭的每個 element 都是\n",
    "# 剛剛定義的 `FakeNewsDataset` 回傳的一個樣本，每個樣本都包含 3 tensors：\n",
    "# - tokens_tensor\n",
    "# - segments_tensor\n",
    "# - label_tensor\n",
    "# 它會對前兩個 tensors 作 zero padding，並產生前面說明過的 masks_tensors\n",
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "    \n",
    "    # 測試集有 labels\n",
    "    if samples[0][2] is not None:\n",
    "        label_ids = torch.stack([s[2] for s in samples])\n",
    "    else:\n",
    "        label_ids = None\n",
    "    \n",
    "    # zero pad 到同一序列長度\n",
    "    tokens_tensors = pad_sequence(tokens_tensors, \n",
    "                                  batch_first=True)\n",
    "    segments_tensors = pad_sequence(segments_tensors, \n",
    "                                    batch_first=True)\n",
    "    \n",
    "    # attention masks，將 tokens_tensors 裡頭不為 zero padding\n",
    "    # 的位置設為 1 讓 BERT 只關注這些位置的 tokens\n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape, \n",
    "                                dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(\n",
    "        tokens_tensors != 0, 1)\n",
    "    \n",
    "    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n",
    "\n",
    "\n",
    "# 初始化一個每次回傳 64 個訓練樣本的 DataLoader\n",
    "# 利用 `collate_fn` 將 list of samples 合併成一個 mini-batch 是關鍵\n",
    "BATCH_SIZE = 1\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE,collate_fn=create_mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tokens_tensors.shape   = torch.Size([1, 57]) \n",
      "tensor([[ 101, 5722, 3300, 3301, 6206, 5310, 2042,  749, 8024,  852, 5381, 1351,\n",
      "         6230, 2533,  800, 6820, 3221, 1469, 3360, 2552, 1963, 3683, 6772, 1394,\n",
      "         6844,  102, 1962, 7318, 6057, 5310, 2042, 5314,  679, 2042, 3184, 4638,\n",
      "         4912, 2269, 2803, 5709, 4413, 8024,  948, 7450, 4638, 4912, 2269, 2957,\n",
      "         3717, 7027, 5010, 1526, 5722, 3300, 3301, 8013,  102]])\n",
      "------------------------\n",
      "segments_tensors.shape = torch.Size([1, 57])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "------------------------\n",
      "masks_tensors.shape    = torch.Size([1, 57])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "------------------------\n",
      "label_ids.shape        = torch.Size([1])\n",
      "tensor([2])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x0000019E7AD14D08>\n",
      "Starting var:.. idx = 0\n",
      "13:52:07.279259 call        16     def __getitem__(self, idx):\n",
      "13:52:07.280243 line        17         if self.mode == \"test\":\n",
      "13:52:07.280243 line        21             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '苏有朋要结婚了，但网友觉得他还是和林心如比较合适'\n",
      "New var:....... text_b = '好闺蜜结婚给不婚族的秦岚扔花球，倒霉的秦岚掉水里笑哭苏有朋！'\n",
      "New var:....... label = 'unrelated'\n",
      "13:52:07.281240 line        23             label_id = self.label_map[label]\n",
      "New var:....... label_id = 2\n",
      "13:52:07.282238 line        24             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(2)\n",
      "13:52:07.282238 line        27         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "13:52:07.282238 line        28         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['苏', '有', '朋', '要', '结', '婚', '了', '，', '但', '网...'还', '是', '和', '林', '心', '如', '比', '较', '合', '适']\n",
      "13:52:07.283234 line        29         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '苏', '有', '朋', '要', '结', '婚', '了', '，'... '和', '林', '心', '如', '比', '较', '合', '适', '[SEP]']\n",
      "13:52:07.284232 line        30         len_a = len(word_pieces)\n",
      "New var:....... len_a = 26\n",
      "13:52:07.284232 line        33         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['好', '闺', '蜜', '结', '婚', '给', '不', '婚', '族', '的...'岚', '掉', '水', '里', '笑', '哭', '苏', '有', '朋', '！']\n",
      "13:52:07.285229 line        34         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '苏', '有', '朋', '要', '结', '婚', '了', '，'... '水', '里', '笑', '哭', '苏', '有', '朋', '！', '[SEP]']\n",
      "13:52:07.285229 line        35         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 31\n",
      "13:52:07.286227 line        38         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 5722, 3300, 3301, 6206, 5310, 2042, 749, 8...7, 7027, 5010, 1526, 5722, 3300, 3301, 8013, 102]\n",
      "13:52:07.287224 line        39         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5722, 3300, 3301, 6206, 5310, 2042... 7027, 5010, 1526, 5722, 3300, 3301, 8013,  102])\n",
      "13:52:07.287224 line        42         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "13:52:07.288222 line        43                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "13:52:07.289219 line        45         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "13:52:07.291217 return      45         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 5722, 3300, 3301, 6206, 5310, 204...1,        1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.014947\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(trainloader))\n",
    "\n",
    "tokens_tensors, segments_tensors, masks_tensors, label_ids = data\n",
    "\n",
    "print(f\"\"\"\n",
    "tokens_tensors.shape   = {tokens_tensors.shape} \n",
    "{tokens_tensors}\n",
    "------------------------\n",
    "segments_tensors.shape = {segments_tensors.shape}\n",
    "{segments_tensors}\n",
    "------------------------\n",
    "masks_tensors.shape    = {masks_tensors.shape}\n",
    "{masks_tensors}\n",
    "------------------------\n",
    "label_ids.shape        = {label_ids.shape}\n",
    "{label_ids}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name            module\n",
      "----------------------\n",
      "embeddings      BertEmbeddings(\n",
      "  (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
      "  (position_embeddings): Embedding(512, 768)\n",
      "  (token_type_embeddings): Embedding(2, 768)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "encoder         BertEncoder(\n",
      "  (layer): ModuleList(\n",
      "    (0): BertLayer(\n",
      "      (attention): BertAttention(\n",
      "        (self): BertSelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output): BertSelfOutput(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): BertIntermediate(\n",
      "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      )\n",
      "      (output): BertOutput(\n",
      "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): BertLayer(\n",
      "      (attention): BertAttention(\n",
      "        (self): BertSelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output): BertSelfOutput(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): BertIntermediate(\n",
      "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      )\n",
      "      (output): BertOutput(\n",
      "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): BertLayer(\n",
      "      (attention): BertAttention(\n",
      "        (self): BertSelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output): BertSelfOutput(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): BertIntermediate(\n",
      "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      )\n",
      "      (output): BertOutput(\n",
      "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): BertLayer(\n",
      "      (attention): BertAttention(\n",
      "        (self): BertSelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output): BertSelfOutput(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): BertIntermediate(\n",
      "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      )\n",
      "      (output): BertOutput(\n",
      "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): BertLayer(\n",
      "      (attention): BertAttention(\n",
      "        (self): BertSelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output): BertSelfOutput(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): BertIntermediate(\n",
      "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      )\n",
      "      (output): BertOutput(\n",
      "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): BertLayer(\n",
      "      (attention): BertAttention(\n",
      "        (self): BertSelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output): BertSelfOutput(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): BertIntermediate(\n",
      "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      )\n",
      "      (output): BertOutput(\n",
      "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): BertLayer(\n",
      "      (attention): BertAttention(\n",
      "        (self): BertSelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output): BertSelfOutput(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): BertIntermediate(\n",
      "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      )\n",
      "      (output): BertOutput(\n",
      "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): BertLayer(\n",
      "      (attention): BertAttention(\n",
      "        (self): BertSelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output): BertSelfOutput(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): BertIntermediate(\n",
      "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      )\n",
      "      (output): BertOutput(\n",
      "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): BertLayer(\n",
      "      (attention): BertAttention(\n",
      "        (self): BertSelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output): BertSelfOutput(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): BertIntermediate(\n",
      "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      )\n",
      "      (output): BertOutput(\n",
      "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): BertLayer(\n",
      "      (attention): BertAttention(\n",
      "        (self): BertSelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output): BertSelfOutput(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): BertIntermediate(\n",
      "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      )\n",
      "      (output): BertOutput(\n",
      "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): BertLayer(\n",
      "      (attention): BertAttention(\n",
      "        (self): BertSelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output): BertSelfOutput(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): BertIntermediate(\n",
      "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      )\n",
      "      (output): BertOutput(\n",
      "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): BertLayer(\n",
      "      (attention): BertAttention(\n",
      "        (self): BertSelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output): BertSelfOutput(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): BertIntermediate(\n",
      "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      )\n",
      "      (output): BertOutput(\n",
      "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "pooler          BertPooler(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (activation): Tanh()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 載入一個可以做中文多分類任務的模型，n_class = 3\n",
    "from transformers import BertForSequenceClassification, BertModel\n",
    "\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-chinese\"\n",
    "\n",
    "\n",
    "\n",
    "model = BertModel.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "\n",
    "\n",
    "# high-level 顯示此模型裡的 modules\n",
    "print(\"\"\"\n",
    "name            module\n",
    "----------------------\"\"\")\n",
    "for name, module in model.named_children():\n",
    "    if name == \"bert\":\n",
    "        for n, _ in module.named_children():\n",
    "            print(f\"{name}:{n}\")\n",
    "    else:\n",
    "        print(\"{:15} {}\".format(name, module))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertConfig\n",
    "\n",
    "# Initializing a BERT bert-base-uncased style configuration\n",
    "configuration = BertConfig(hidden_size=768)\n",
    "\n",
    "# Initializing a model from the bert-base-uncased style configuration\n",
    "model = BertModel(configuration)\n",
    "\n",
    "# Accessing the model configuration\n",
    "#configuration = model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n",
      "data :  (tensor([[ 101, 5722, 3300, 3301, 6206, 5310, 2042,  749, 8024,  852, 5381, 1351,\n",
      "         6230, 2533,  800, 6820, 3221, 1469, 3360, 2552, 1963, 3683, 6772, 1394,\n",
      "         6844,  102, 1962, 7318, 6057, 5310, 2042, 5314,  679, 2042, 3184, 4638,\n",
      "         4912, 2269, 2803, 5709, 4413, 8024,  948, 7450, 4638, 4912, 2269, 2957,\n",
      "         3717, 7027, 5010, 1526, 5722, 3300, 3301, 8013,  102]]), tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1]]), tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1]]), tensor([2]))\n",
      "(tensor([[[-0.9314,  1.7674, -0.2113,  ..., -0.1514, -0.7655,  0.4663],\n",
      "         [ 0.2299,  2.0348,  0.1942,  ..., -0.5772, -1.0472,  0.6396],\n",
      "         [-0.1634,  1.5729,  0.3137,  ...,  0.1404, -0.9214, -0.7449],\n",
      "         ...,\n",
      "         [ 1.5928,  0.3425, -0.9119,  ..., -1.7611, -0.9512,  0.9662],\n",
      "         [-1.4065,  0.6669, -1.1481,  ..., -3.2651, -0.2221, -0.1400],\n",
      "         [ 0.7680,  0.0343, -1.2727,  ..., -1.2612,  0.8742,  0.4438]]]), tensor([[ 3.5098e-01, -7.9196e-01, -2.1698e-01, -3.6640e-01,  1.5158e-01,\n",
      "         -1.5165e-01, -1.0893e-01, -1.3903e-01, -5.2109e-01, -6.1910e-01,\n",
      "         -8.2561e-01,  4.2614e-01,  6.9227e-01,  2.9236e-01,  1.8154e-01,\n",
      "          7.4684e-02, -6.9392e-01, -6.1038e-01,  4.2819e-01, -3.6064e-01,\n",
      "          7.5615e-01, -1.5883e-01,  8.2739e-01,  7.6967e-02, -5.1764e-01,\n",
      "         -5.2033e-01,  5.8470e-01,  3.2987e-01,  5.3080e-02, -6.0996e-01,\n",
      "          7.0961e-02, -2.9388e-01,  5.0768e-01, -5.1933e-01, -7.2867e-01,\n",
      "         -9.4709e-02, -6.2928e-01, -5.6185e-01,  1.2785e-01, -5.2781e-01,\n",
      "          8.2436e-01,  8.9368e-01,  7.9929e-01, -3.8593e-01, -2.2878e-01,\n",
      "          5.1985e-01, -1.5469e-01, -5.7444e-01, -5.4170e-01, -4.2583e-01,\n",
      "         -6.7562e-01,  5.3583e-01, -6.9473e-01, -7.5306e-02, -3.6913e-01,\n",
      "          2.6101e-01,  4.1753e-01,  1.9410e-01, -1.9219e-01,  3.2181e-01,\n",
      "         -3.3338e-01,  2.8422e-01, -1.8168e-02, -6.1339e-01,  3.0172e-01,\n",
      "         -7.9172e-02, -3.7844e-01,  4.8809e-02, -4.0456e-01, -2.4728e-01,\n",
      "         -6.1096e-01,  9.0753e-01,  5.4195e-01,  7.1230e-01, -3.3502e-01,\n",
      "         -2.3000e-01,  3.7615e-01,  4.7040e-01, -1.5759e-01, -2.2705e-01,\n",
      "          8.8663e-01, -6.9718e-01, -3.7398e-01,  3.5992e-01,  5.8475e-01,\n",
      "          3.6429e-01, -3.3474e-01,  1.9060e-01, -2.5432e-01, -1.3011e-01,\n",
      "          2.9987e-01, -4.9210e-01,  6.4743e-01, -5.3831e-01, -1.1968e-01,\n",
      "         -4.8547e-01, -4.2854e-01, -6.4268e-01, -4.5875e-01,  1.9033e-01,\n",
      "         -1.3350e-03,  5.4591e-01,  4.9725e-02,  6.7051e-02, -4.0489e-01,\n",
      "          2.2194e-02,  3.6957e-02,  3.7248e-01, -6.9220e-01,  6.8714e-01,\n",
      "         -4.3666e-01,  8.9837e-01,  5.5008e-01, -1.4887e-01, -1.7441e-01,\n",
      "         -8.4402e-01,  2.5469e-01, -2.8564e-04, -4.7952e-01, -2.2976e-01,\n",
      "         -8.6973e-01,  2.8407e-01,  6.8879e-01, -2.7601e-01, -3.6474e-01,\n",
      "          1.9948e-01, -3.5726e-01,  3.4889e-02, -7.7024e-03, -1.6495e-01,\n",
      "         -5.4091e-01,  4.4815e-01, -4.7388e-02,  2.3572e-01,  2.0052e-01,\n",
      "         -3.1437e-01, -5.4782e-01,  7.1258e-01, -7.8987e-01,  5.6568e-01,\n",
      "          3.0758e-01,  1.2519e-01,  4.3610e-01, -7.7203e-01,  4.5649e-01,\n",
      "          9.1293e-01,  6.2868e-01, -2.3879e-01, -1.0734e-01, -2.1725e-01,\n",
      "          4.3750e-01, -5.6037e-01, -5.5928e-01,  7.8506e-01,  1.5227e-01,\n",
      "          2.1948e-01, -6.0718e-01,  4.0384e-01,  2.3270e-01, -6.1636e-01,\n",
      "          1.5734e-01,  1.8240e-01,  3.8397e-01, -4.5162e-01, -3.5871e-01,\n",
      "         -3.5716e-01,  2.7109e-01, -4.0267e-01,  4.3118e-01, -6.2521e-01,\n",
      "          6.8180e-01,  3.9506e-01, -3.6915e-01,  8.9484e-02,  1.3949e-01,\n",
      "          5.1115e-02,  6.8244e-01,  5.8933e-01,  1.0529e-01, -3.4922e-01,\n",
      "         -4.3984e-01,  8.4169e-01, -4.4078e-01, -3.2352e-01, -4.3346e-01,\n",
      "         -3.2477e-02,  4.3369e-01, -1.8710e-01, -6.4703e-02, -9.8384e-02,\n",
      "         -5.4549e-01,  7.0178e-02, -5.7347e-02, -2.6738e-01,  1.7262e-01,\n",
      "         -2.0519e-01, -3.6862e-01, -3.0231e-01, -3.2130e-01, -4.3925e-01,\n",
      "         -4.5637e-01, -2.9794e-01, -5.5802e-01, -1.9393e-01, -2.9313e-01,\n",
      "         -1.8532e-01,  1.2715e-01, -7.0972e-01, -1.9119e-01, -2.8819e-03,\n",
      "         -8.5882e-02,  1.1690e-01, -4.1385e-01, -5.6492e-01, -8.4818e-01,\n",
      "          4.3850e-02, -1.5093e-01, -7.7832e-01, -6.0856e-01,  3.3055e-01,\n",
      "         -6.3055e-01, -2.3238e-01, -6.2837e-01,  1.8323e-01, -4.3737e-01,\n",
      "         -4.1048e-01, -4.0798e-01, -1.1985e-01,  6.6828e-01,  3.8634e-01,\n",
      "         -4.4049e-01,  2.3730e-01,  5.5947e-01,  8.2090e-01,  3.6245e-01,\n",
      "          3.5925e-01, -5.5825e-01,  9.4039e-02,  1.2942e-01,  3.5832e-01,\n",
      "          4.9910e-01,  5.7864e-01, -2.7018e-01, -7.0465e-01, -5.5802e-01,\n",
      "         -3.8323e-01,  3.5038e-01, -2.0026e-01, -5.7911e-01,  1.8491e-01,\n",
      "         -2.6050e-03, -4.3303e-01,  1.0563e-01, -4.8342e-02, -4.5493e-01,\n",
      "          4.6044e-01,  6.6182e-01,  3.0930e-01, -7.2389e-01,  2.4226e-01,\n",
      "         -4.9942e-02, -1.5566e-01, -3.8945e-01, -5.7491e-01,  3.9966e-01,\n",
      "          3.0629e-01,  3.8343e-01, -4.1176e-01, -6.0760e-02, -5.5877e-01,\n",
      "         -1.7933e-01,  4.1627e-02,  7.5110e-01, -4.8608e-01, -5.8269e-01,\n",
      "          1.8489e-01,  5.7009e-01,  1.1991e-01, -7.7456e-01, -4.2906e-01,\n",
      "          8.0675e-02,  1.0144e-01, -3.4129e-01,  1.9680e-01, -1.7266e-01,\n",
      "          9.8654e-02, -9.2284e-02, -2.1743e-01, -3.6409e-01, -1.0177e-01,\n",
      "         -4.6248e-01, -4.3203e-01,  3.2323e-01, -2.6742e-01,  6.9620e-01,\n",
      "          7.4607e-01, -2.2525e-01, -7.6887e-02, -5.5952e-02,  5.2897e-01,\n",
      "          4.1456e-01,  5.8856e-01,  2.7749e-01,  2.5398e-02, -1.5924e-01,\n",
      "          4.7486e-01, -8.3818e-02, -4.0744e-01, -5.9377e-01, -3.8218e-02,\n",
      "         -3.5377e-01, -2.9216e-01,  4.7541e-01, -4.4247e-01,  2.0902e-01,\n",
      "          2.5784e-01, -4.8433e-01,  2.6353e-02, -2.0941e-01, -8.5506e-01,\n",
      "          5.8952e-01,  1.2598e-01, -1.6827e-01,  3.0171e-01, -1.7876e-01,\n",
      "          9.6338e-02,  6.1957e-01, -2.5432e-01,  7.1791e-01, -6.8000e-01,\n",
      "         -5.0073e-01, -2.2851e-01, -3.6789e-01, -5.5163e-01, -8.4586e-01,\n",
      "          5.9001e-01, -2.3382e-02,  1.7963e-01, -9.8437e-02,  7.7401e-01,\n",
      "          2.1529e-01, -1.6289e-01, -6.8845e-02, -2.8173e-01, -4.4648e-01,\n",
      "          8.6609e-01, -3.6337e-02,  6.0904e-01, -4.6477e-01,  1.2102e-01,\n",
      "         -3.7115e-01, -4.7791e-01,  5.1706e-01,  3.8695e-01,  1.9414e-01,\n",
      "          4.4117e-01,  1.7452e-01, -5.1833e-01,  4.1244e-01,  4.2773e-01,\n",
      "          4.7535e-01,  1.1045e-01,  7.2124e-01,  7.5510e-01, -2.5036e-01,\n",
      "          2.4206e-01, -4.6992e-01, -8.7217e-01, -1.8835e-01,  5.2210e-01,\n",
      "         -1.0989e-01,  7.3713e-01, -6.1768e-02, -3.4386e-01,  1.7869e-01,\n",
      "         -3.9527e-01, -5.5508e-01,  8.0637e-01,  7.6349e-01, -8.0182e-01,\n",
      "          4.1115e-01,  3.7431e-01, -1.0505e-01,  4.7950e-01,  8.3477e-01,\n",
      "         -1.0277e-01,  3.9554e-01,  7.1016e-01, -6.1716e-01,  4.4793e-01,\n",
      "         -3.7723e-01,  5.7090e-01,  1.9876e-01,  2.8916e-01, -4.2085e-01,\n",
      "         -6.9739e-01, -4.8368e-01, -3.2247e-01, -4.5183e-01,  4.5750e-01,\n",
      "         -5.7021e-01,  1.7998e-01,  5.8314e-01, -1.6072e-01, -6.7173e-01,\n",
      "         -2.8382e-01,  7.9528e-01, -7.9265e-01, -5.8912e-01,  2.5393e-01,\n",
      "         -1.9077e-01, -6.0997e-01,  5.0068e-01, -6.1262e-01,  7.0891e-01,\n",
      "          3.4854e-01, -3.2512e-02, -1.8983e-01,  5.4062e-01,  4.8206e-01,\n",
      "         -5.2029e-01, -3.5772e-01,  1.0763e-01,  3.5351e-01, -5.0234e-02,\n",
      "          7.3253e-01, -2.3443e-01, -2.4486e-02,  5.3277e-01,  2.7049e-01,\n",
      "          6.6168e-01, -6.0431e-01, -5.8691e-01,  5.5588e-01, -1.7614e-01,\n",
      "          1.3820e-01, -4.1915e-01, -2.9325e-01,  1.9768e-01,  2.7523e-01,\n",
      "         -5.6706e-01,  3.5375e-01, -2.2460e-01,  5.4318e-01,  2.1654e-01,\n",
      "          7.0419e-01,  4.1587e-01,  2.7984e-01,  1.4743e-01,  4.0999e-01,\n",
      "          5.5393e-01, -6.3947e-01,  2.6227e-01,  9.2530e-02, -3.4299e-01,\n",
      "          5.0768e-01,  6.1484e-01, -7.8318e-01, -1.7498e-01, -2.2992e-01,\n",
      "          4.0826e-01,  5.6007e-01, -1.7187e-01, -2.2483e-01, -6.7077e-02,\n",
      "          2.2556e-01, -7.8773e-02,  1.3923e-01, -2.2293e-01, -4.8194e-01,\n",
      "          7.3855e-01,  5.8134e-01, -3.9977e-01,  3.9044e-01, -7.4894e-02,\n",
      "          8.2436e-02, -3.8745e-01, -8.5639e-02,  8.5372e-02, -6.5443e-01,\n",
      "          3.7176e-01,  2.7464e-01,  4.8376e-01, -3.7761e-01,  5.6493e-01,\n",
      "          7.5447e-01,  8.3260e-02, -3.2772e-01,  3.6153e-01,  5.6866e-01,\n",
      "          1.4975e-02,  7.2413e-01,  1.1060e-01, -4.4575e-01, -4.6223e-01,\n",
      "         -8.0331e-01,  2.4476e-01, -2.1366e-01, -1.7711e-01,  2.3230e-01,\n",
      "          8.0701e-01, -4.4042e-01,  3.5011e-01,  2.9670e-01, -5.0822e-01,\n",
      "         -1.0955e-01, -4.9650e-01, -2.3165e-02, -4.0694e-01, -1.4953e-01,\n",
      "          1.1821e-01,  2.9479e-01, -8.8049e-01, -2.8683e-01,  3.0059e-01,\n",
      "         -3.5271e-01, -3.1935e-01,  6.1359e-02,  4.2632e-01, -5.6460e-01,\n",
      "         -7.0817e-01,  6.0859e-01, -2.3666e-01,  7.3522e-01, -2.4866e-01,\n",
      "         -2.9422e-02, -9.8998e-02,  2.0540e-01,  4.9871e-01, -1.1235e-01,\n",
      "         -4.3534e-01,  5.9235e-01, -3.2061e-01, -4.3555e-02,  4.4142e-01,\n",
      "         -7.3175e-01,  5.4400e-01, -5.8719e-01, -4.6394e-01,  2.5077e-03,\n",
      "         -9.0473e-01,  1.8672e-01, -3.0655e-01, -1.5455e-01,  7.1617e-02,\n",
      "          6.6463e-01, -6.0649e-01, -2.7895e-01, -7.7131e-01,  1.1241e-01,\n",
      "          1.3028e-01, -4.9558e-01, -8.9176e-01, -3.8773e-02,  3.8777e-01,\n",
      "         -7.1583e-01,  3.6560e-01, -2.7658e-01, -5.2963e-01, -8.4277e-02,\n",
      "         -2.1103e-02,  3.1964e-01,  5.1486e-02,  8.4422e-01,  4.3060e-01,\n",
      "          6.0751e-01,  5.8525e-01,  1.2979e-01, -7.5176e-01,  6.4309e-02,\n",
      "         -6.4046e-01,  1.6601e-01, -1.6850e-01,  3.3085e-01,  5.2002e-01,\n",
      "          3.7140e-01, -2.5078e-01, -6.5751e-01,  1.2499e-01,  3.1016e-01,\n",
      "         -7.6820e-01,  4.4278e-02,  6.8665e-02, -4.7163e-01, -1.5290e-01,\n",
      "          3.3138e-01,  6.6039e-01,  7.2574e-01,  3.4528e-03, -8.7543e-01,\n",
      "         -5.3682e-01, -3.6858e-01,  8.8976e-02, -1.7942e-01, -5.0986e-01,\n",
      "          2.5972e-01,  1.2888e-01, -6.0832e-02,  8.0853e-01,  3.4906e-01,\n",
      "          3.7015e-01, -6.5724e-01,  7.1556e-01, -4.7973e-01, -1.2094e-01,\n",
      "         -4.5193e-01, -3.8882e-01, -2.4535e-01,  2.4195e-01, -4.5718e-01,\n",
      "         -5.6169e-02,  5.5522e-02,  2.1639e-01, -1.6909e-01,  6.6829e-01,\n",
      "         -1.0079e-01,  4.8842e-01, -7.9480e-02, -8.2027e-01, -4.7512e-01,\n",
      "          3.0729e-01, -2.6621e-01,  6.2894e-01,  6.9960e-02,  2.0000e-01,\n",
      "          8.1740e-01,  5.6873e-01,  8.9102e-01, -5.3734e-01,  4.3909e-01,\n",
      "         -2.2533e-01, -4.2914e-01, -5.1384e-01,  9.0509e-02,  4.8844e-01,\n",
      "          2.5741e-01,  7.8796e-01, -3.7239e-01,  1.7399e-01,  6.1184e-01,\n",
      "         -3.8732e-01,  2.2962e-01, -3.8368e-02, -1.2208e-01, -8.4188e-01,\n",
      "          7.7880e-01,  6.8595e-01,  2.3518e-01, -1.9370e-02,  7.5326e-01,\n",
      "          2.0356e-01, -5.1931e-01,  1.1107e-01,  2.1855e-01, -6.7278e-01,\n",
      "         -1.0872e-01, -5.5397e-01,  2.1385e-01, -1.2451e-03, -1.7967e-02,\n",
      "          3.3863e-01,  8.8965e-01,  1.5200e-01,  2.7311e-01,  3.7653e-01,\n",
      "         -7.1380e-01,  1.6678e-01,  4.6528e-01, -4.2136e-01,  7.5002e-01,\n",
      "          2.9002e-01,  3.4358e-01, -3.5317e-01, -9.0789e-02, -5.3798e-01,\n",
      "         -1.0062e-01, -4.3579e-01, -1.1643e-01, -7.1553e-01, -2.7155e-01,\n",
      "         -7.0832e-01,  6.4915e-01, -7.7298e-01,  2.2276e-01, -7.7371e-01,\n",
      "         -7.9219e-02, -4.3705e-01, -5.4553e-01,  8.4737e-02,  2.1456e-01,\n",
      "          2.0207e-01,  4.3949e-01, -1.7827e-01,  6.1931e-01,  2.4582e-01,\n",
      "          9.9395e-02, -4.9635e-01,  4.5589e-01,  3.1786e-01, -1.4429e-01,\n",
      "          8.4958e-01,  5.3949e-01,  4.2001e-01,  8.8523e-02, -7.7147e-01,\n",
      "         -5.5243e-02, -9.5628e-02,  3.1315e-01, -3.4367e-02, -6.9749e-01,\n",
      "         -1.2654e-01,  2.6446e-01,  1.2145e-01,  7.4341e-02,  8.6312e-02,\n",
      "          4.8198e-02,  6.4779e-01, -6.9199e-01, -2.9954e-01, -4.8915e-01,\n",
      "         -1.5778e-01,  8.5216e-01,  2.6583e-01, -3.4117e-02, -7.5959e-02,\n",
      "          3.8816e-01, -4.7570e-01,  8.1654e-01, -4.5763e-02,  7.7568e-02,\n",
      "         -4.1905e-01,  6.2264e-01, -7.3777e-01,  6.2133e-02, -4.9801e-03,\n",
      "          4.0862e-01, -4.5146e-01,  3.8809e-01, -7.1952e-01, -5.6765e-01,\n",
      "          9.9291e-02, -3.8054e-01, -1.6740e-02,  3.3296e-02, -8.6955e-02,\n",
      "          4.0110e-01, -4.8812e-01, -4.3007e-01,  6.6260e-01, -4.8796e-01,\n",
      "         -1.9869e-01, -5.8544e-01, -2.3456e-01, -2.7236e-02,  2.5671e-02,\n",
      "          7.5429e-01,  3.7757e-01, -2.6806e-01,  1.5359e-01, -2.2283e-01,\n",
      "          5.9651e-01, -1.8393e-01,  6.7392e-01,  1.2476e-01,  7.8564e-01,\n",
      "          1.3999e-01,  7.4512e-01, -3.5467e-01]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x0000019E7AD14D08>\n",
      "Starting var:.. idx = 0\n",
      "13:52:12.080857 call        16     def __getitem__(self, idx):\n",
      "13:52:12.081873 line        17         if self.mode == \"test\":\n",
      "13:52:12.081873 line        21             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '苏有朋要结婚了，但网友觉得他还是和林心如比较合适'\n",
      "New var:....... text_b = '好闺蜜结婚给不婚族的秦岚扔花球，倒霉的秦岚掉水里笑哭苏有朋！'\n",
      "New var:....... label = 'unrelated'\n",
      "13:52:12.081873 line        23             label_id = self.label_map[label]\n",
      "New var:....... label_id = 2\n",
      "13:52:12.081873 line        24             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(2)\n",
      "13:52:12.081873 line        27         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "13:52:12.082852 line        28         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['苏', '有', '朋', '要', '结', '婚', '了', '，', '但', '网...'还', '是', '和', '林', '心', '如', '比', '较', '合', '适']\n",
      "13:52:12.082852 line        29         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '苏', '有', '朋', '要', '结', '婚', '了', '，'... '和', '林', '心', '如', '比', '较', '合', '适', '[SEP]']\n",
      "13:52:12.082852 line        30         len_a = len(word_pieces)\n",
      "New var:....... len_a = 26\n",
      "13:52:12.083849 line        33         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['好', '闺', '蜜', '结', '婚', '给', '不', '婚', '族', '的...'岚', '掉', '水', '里', '笑', '哭', '苏', '有', '朋', '！']\n",
      "13:52:12.083849 line        34         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '苏', '有', '朋', '要', '结', '婚', '了', '，'... '水', '里', '笑', '哭', '苏', '有', '朋', '！', '[SEP]']\n",
      "13:52:12.084847 line        35         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 31\n",
      "13:52:12.084847 line        38         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 5722, 3300, 3301, 6206, 5310, 2042, 749, 8...7, 7027, 5010, 1526, 5722, 3300, 3301, 8013, 102]\n",
      "13:52:12.085847 line        39         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5722, 3300, 3301, 6206, 5310, 2042... 7027, 5010, 1526, 5722, 3300, 3301, 8013,  102])\n",
      "13:52:12.085847 line        42         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "13:52:12.087840 line        43                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "13:52:12.088837 line        45         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "13:52:12.089834 return      45         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 5722, 3300, 3301, 6206, 5310, 204...1,        1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.010972\n"
     ]
    }
   ],
   "source": [
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "      \n",
    "    with torch.no_grad():\n",
    "        # 遍巡整個資料集\n",
    "        for data in dataloader:\n",
    "            # 將所有 tensors 移到 GPU 上\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
    "            \n",
    "            print('data : ',data)\n",
    "            # 別忘記前 3 個 tensors 分別為 tokens, segments 以及 masks\n",
    "            # 且強烈建議在將這些 tensors 丟入 `model` 時指定對應的參數名稱\n",
    "            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors)\n",
    "            print(outputs)\n",
    "            logits = outputs[0]\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            return outputs\n",
    "            # 用來計算訓練集的分類準確率\n",
    "            \n",
    "            if compute_acc:\n",
    "                labels = data[3]\n",
    "                total += labels.size(0)\n",
    "                correct += (pred == labels).sum().item()\n",
    "                \n",
    "            # 將當前 batch 記錄下來\n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "    \n",
    "    if compute_acc:\n",
    "        acc = correct / total\n",
    "        return predictions, acc\n",
    "    return predictions\n",
    "\n",
    "# 讓模型跑在 GPU 上並取得訓練集的分類準確率\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "#model = model.to(device)\n",
    "acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "#print(\"classification acc:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7.6800e-01,  3.4254e-02, -1.2727e+00, -1.6715e+00,  1.3263e+00,\n",
       "         1.5190e+00, -8.8432e-01,  6.5781e-01,  1.8409e+00,  8.7262e-01,\n",
       "         5.3732e-01,  6.1185e-01, -3.5615e-01, -5.1365e-02,  2.2069e+00,\n",
       "         1.6405e+00,  3.2211e-01,  9.5048e-01,  2.1713e+00, -7.3055e-01,\n",
       "        -1.0986e+00, -1.9607e-01, -6.7902e-02, -8.4460e-02,  2.4786e-03,\n",
       "         1.7666e+00, -1.0979e+00,  1.4223e+00,  6.2535e-01,  1.8184e+00,\n",
       "         1.3699e+00,  1.3204e-01,  8.5773e-02, -9.0599e-01,  1.6741e-01,\n",
       "        -7.2120e-02, -1.7546e+00, -1.0835e+00, -1.7812e-02, -7.7225e-01,\n",
       "         1.6390e+00, -1.9652e+00, -2.6975e+00, -8.4246e-02,  4.9995e-01,\n",
       "         9.7035e-01,  5.6937e-01,  2.3565e+00, -6.5288e-01,  7.8862e-01,\n",
       "         1.3754e-01, -6.8679e-02, -1.0387e+00,  7.9469e-01, -8.5058e-01,\n",
       "         1.1181e+00,  1.5910e+00, -2.0745e+00,  3.6740e-01, -1.8472e+00,\n",
       "        -4.9975e-01,  8.8842e-01, -7.7970e-01,  4.2002e-01,  4.0560e-01,\n",
       "         1.1857e+00,  1.0728e+00,  5.2458e-01, -9.1254e-02,  2.2146e+00,\n",
       "         6.1102e-01, -1.3409e+00, -1.6562e+00, -1.2655e+00,  1.0524e+00,\n",
       "         4.5260e-01,  2.5429e-01,  2.3858e-01,  2.3725e+00,  1.7935e+00,\n",
       "        -1.3658e+00,  1.0683e+00,  1.2832e+00,  3.2748e-01,  1.7745e-01,\n",
       "        -1.8540e+00, -1.1624e+00,  1.4194e+00,  8.0852e-01, -1.9558e+00,\n",
       "        -1.5007e+00,  1.0917e+00, -1.5837e+00,  1.4774e+00,  5.0124e-01,\n",
       "        -1.4355e+00,  8.4059e-01,  2.2367e+00, -3.6914e-01, -2.8598e-01,\n",
       "        -5.0118e-01, -5.8228e-01, -8.8692e-02,  3.5666e-01, -6.3339e-02,\n",
       "        -3.2675e-01, -1.0029e-01,  5.7128e-01, -7.5506e-01,  4.8472e-01,\n",
       "         7.4856e-01, -1.1425e+00,  3.5364e-01, -3.6859e-01, -2.8390e-01,\n",
       "        -1.3124e+00,  5.8683e-01, -4.1177e-02, -1.4781e+00, -1.2007e+00,\n",
       "         1.9025e+00,  4.4042e-01,  2.3439e-01,  3.9074e-01,  7.3685e-01,\n",
       "         1.2493e+00,  1.8995e+00, -3.4278e-01, -1.4687e+00, -5.1580e-01,\n",
       "         3.9471e-01, -3.7972e-01,  6.6430e-01, -7.2284e-01, -2.0115e-02,\n",
       "         2.2470e-01,  1.0429e-01, -6.4460e-01, -6.6582e-01,  4.8416e-01,\n",
       "         4.4042e-01, -6.4454e-01,  1.6186e+00,  9.0279e-01, -1.6269e-01,\n",
       "        -9.4996e-02,  7.9876e-02,  1.0204e+00,  6.6927e-01, -3.6430e-01,\n",
       "         9.4540e-02, -1.8613e-01,  2.3021e-01,  1.1037e+00, -6.2027e-01,\n",
       "        -4.5268e-01, -2.3065e-01,  1.3060e+00,  1.6099e+00,  5.2158e-01,\n",
       "        -4.2880e-01,  2.3098e+00, -1.1731e+00, -3.1412e-01,  6.2444e-01,\n",
       "        -1.0481e+00, -1.3300e+00,  2.1974e+00,  1.5150e+00,  4.5644e-01,\n",
       "         3.3506e-01, -6.8155e-01,  1.6568e-02, -8.0368e-02,  1.9136e-01,\n",
       "        -9.9152e-01,  1.3462e-01, -5.5179e-01, -1.5891e+00,  3.1363e-01,\n",
       "         1.4269e+00, -4.9607e-01,  1.4795e-01, -1.4607e+00, -5.4742e-01,\n",
       "        -7.1645e-02, -2.5725e+00, -8.8418e-01,  7.5527e-01, -4.8849e-01,\n",
       "         1.2627e+00,  1.0618e+00, -7.8148e-02, -1.0921e+00, -1.9402e+00,\n",
       "        -2.1726e+00,  9.4410e-01, -9.9545e-01, -1.5456e+00, -2.3781e-01,\n",
       "        -9.7471e-01, -4.0351e-01,  7.8124e-01, -1.6619e-01, -1.2862e+00,\n",
       "        -3.9568e-01,  9.2163e-01,  1.0800e+00,  2.1177e-01, -3.6750e-01,\n",
       "        -1.0891e+00,  8.5222e-02,  7.9961e-02, -2.2297e+00,  3.5602e-01,\n",
       "         8.0627e-01,  5.7094e-01,  3.0638e-01, -9.1937e-01, -6.9388e-01,\n",
       "        -6.5035e-01, -5.5857e-01,  7.6606e-01,  1.8509e+00,  1.1109e+00,\n",
       "        -5.1759e-03, -6.0969e-01, -5.6371e-01, -6.3955e-03, -1.6693e-01,\n",
       "        -7.7909e-01, -1.7319e+00,  1.0090e+00,  5.2812e-01, -3.8069e-01,\n",
       "        -1.7377e+00,  1.4255e-01,  1.0999e+00, -6.0050e-02,  7.7771e-01,\n",
       "        -6.3342e-02, -1.2201e-01, -4.9297e-01, -7.4751e-01,  5.0261e-01,\n",
       "        -5.4572e-01,  1.5814e+00, -2.1641e-01, -1.8149e+00, -1.0087e+00,\n",
       "         2.8027e+00, -2.6408e+00,  5.4240e-01, -6.2697e-01,  1.5863e+00,\n",
       "         2.1645e+00, -4.5729e-01, -2.0518e-01, -1.7891e+00, -3.1566e-01,\n",
       "         7.2459e-01, -9.2123e-02,  1.1963e+00,  9.0526e-01, -5.7338e-01,\n",
       "        -2.4117e+00, -8.1111e-01, -1.5401e+00, -1.9249e+00,  5.8153e-01,\n",
       "         4.7346e-01, -1.2463e+00,  4.0049e-01, -9.0779e-01, -3.6650e-01,\n",
       "         1.7387e+00,  1.6430e+00,  8.0972e-03,  1.0036e-01,  2.2207e-01,\n",
       "         1.3065e+00,  1.5049e+00,  1.0971e-01, -1.5692e+00,  1.2098e+00,\n",
       "        -3.1388e-01, -7.4321e-01,  4.3360e-01,  5.4943e-02, -3.8702e-01,\n",
       "        -8.5941e-01,  1.1684e+00,  2.2423e+00,  6.1927e-01, -7.4301e-02,\n",
       "        -1.0306e+00, -5.0960e-01, -1.5390e-01,  8.5924e-01, -5.0127e-02,\n",
       "         9.5275e-01, -9.7098e-01, -7.4417e-01,  5.0336e-01,  9.8734e-01,\n",
       "         4.5638e-01, -7.5912e-01,  6.8275e-01, -1.0270e+00, -5.1020e-01,\n",
       "         8.9933e-01,  2.5725e-01,  1.2761e-01,  8.7782e-01,  2.6274e-01,\n",
       "        -1.9887e-01,  9.7971e-01, -1.1010e+00, -8.3619e-01, -7.3601e-01,\n",
       "        -2.9271e-02, -1.6470e-01, -8.9136e-01,  4.6033e-01,  5.4802e-01,\n",
       "         8.2922e-01, -9.2895e-01, -4.8200e-02, -1.2113e+00, -3.7104e-01,\n",
       "        -7.5571e-02, -6.3054e-01,  6.6994e-01,  6.0431e-01, -4.1004e-01,\n",
       "         6.6942e-01, -1.1565e+00,  6.0130e-02,  1.1565e+00, -5.6935e-01,\n",
       "         1.4043e-01,  7.4174e-01, -9.5367e-01, -4.7781e-01, -6.1405e-01,\n",
       "        -2.7807e-01, -8.5503e-01, -1.0115e+00, -2.3944e-01,  1.2224e+00,\n",
       "        -8.8108e-01,  3.6561e-01, -8.3961e-01,  3.5353e-02,  1.2221e+00,\n",
       "        -3.1351e-01,  6.6779e-01, -6.0322e-01,  4.3446e-01, -1.1754e+00,\n",
       "         8.5886e-01,  6.1033e-01, -9.8773e-01, -3.6511e-01,  2.1710e-01,\n",
       "        -1.4258e+00,  3.9198e-01,  8.0238e-01,  8.0615e-01,  3.2025e-01,\n",
       "         1.2612e-01,  3.1809e-01,  1.7700e-01,  2.2798e-01, -3.6015e-01,\n",
       "        -1.6241e+00,  3.8160e-01, -2.0550e+00, -3.2265e+00, -3.0188e-01,\n",
       "        -1.1438e+00,  3.0385e-01, -1.1639e+00, -1.2118e+00, -1.2565e-01,\n",
       "         4.5897e-01,  3.2841e-01, -3.7708e-01,  6.7861e-02,  1.9405e+00,\n",
       "         2.9637e-01,  3.4385e-01,  6.8227e-01,  1.1797e+00, -8.6802e-01,\n",
       "         3.6750e-02,  7.4852e-01, -9.2062e-01,  1.4899e+00, -2.2735e+00,\n",
       "        -3.2541e-01, -2.3290e-01,  1.0145e+00,  9.8007e-01,  3.6533e-02,\n",
       "         7.7823e-01,  7.0093e-01, -4.3433e-01,  9.8748e-02, -3.1268e-01,\n",
       "        -3.5335e-01, -1.2989e+00,  1.0486e-01,  8.2358e-01, -7.1259e-01,\n",
       "        -3.4381e-02,  1.6813e+00, -9.6194e-01,  1.5471e+00,  1.0978e-01,\n",
       "        -1.9093e+00,  6.9028e-01,  9.6250e-01, -8.3310e-01,  1.0156e+00,\n",
       "        -1.3394e+00, -7.5072e-01,  5.1732e-01, -3.4650e-01, -8.6876e-01,\n",
       "         4.2999e-01,  4.3594e-01,  1.3227e+00,  6.3709e-01,  3.8642e-01,\n",
       "        -1.0170e+00, -3.8483e-01, -3.0199e-01,  4.8617e-01, -3.4028e-01,\n",
       "        -8.7542e-01, -2.3312e-01,  2.5241e-01, -6.9570e-01, -5.8436e-01,\n",
       "        -1.2057e+00,  1.4380e+00, -1.8786e+00,  2.6677e-01,  2.4417e-01,\n",
       "         1.7990e-01, -4.3691e-01,  1.4562e-01,  1.0424e+00,  8.4896e-01,\n",
       "         5.8998e-02, -2.0654e-01,  4.6830e-01,  1.6213e-02,  1.0885e+00,\n",
       "        -5.4753e-01, -1.3133e+00, -4.3151e-01, -6.6743e-01, -1.6940e+00,\n",
       "        -2.4145e-01, -2.0677e-01,  6.4494e-01, -3.4228e-01,  1.0908e+00,\n",
       "        -5.5371e-01, -1.1024e+00,  5.2104e-01,  1.0417e+00,  9.7786e-01,\n",
       "        -6.4539e-01,  3.7345e-01,  6.4837e-01, -7.6360e-01, -2.4309e-01,\n",
       "        -6.3367e-01,  2.5142e+00,  9.5941e-01,  1.0996e+00,  3.8611e-02,\n",
       "         8.6635e-01, -3.2277e-01,  1.5334e+00, -6.4057e-01,  1.3079e+00,\n",
       "        -1.7966e+00, -2.6130e-01, -1.4621e+00,  1.8351e+00, -6.8069e-02,\n",
       "        -7.5580e-01, -1.1315e+00, -1.7261e-01,  6.1781e-01, -2.0441e-01,\n",
       "         4.5537e-01,  6.3185e-01, -8.9597e-01, -3.5398e-01, -1.1387e+00,\n",
       "        -1.2151e-01, -1.1654e+00,  2.0047e-01,  2.9283e-01, -4.2031e-01,\n",
       "         1.0262e+00, -1.4411e-01,  1.8484e+00,  1.4349e-01,  1.3065e+00,\n",
       "        -1.6710e+00,  6.2343e-01,  5.7532e-01,  2.0247e-01,  4.2803e-01,\n",
       "         1.7955e-01, -6.0401e-01, -1.4144e+00, -4.1329e-01,  1.1779e-01,\n",
       "         5.2196e-02,  2.3981e-01, -5.5567e-01, -8.3004e-01, -5.7788e-02,\n",
       "        -4.2083e-01,  3.6045e-01, -4.8373e-02, -1.0898e-01,  1.2299e-01,\n",
       "         6.5732e-01,  2.2224e-02,  3.4043e-01, -2.1406e-01, -4.7035e-01,\n",
       "        -7.4022e-01,  1.0871e+00,  4.7885e-01,  1.5221e+00,  2.1279e-02,\n",
       "        -1.3728e+00, -9.6050e-01,  1.1398e+00,  5.0560e-02,  8.6875e-01,\n",
       "         1.0818e+00,  2.6279e+00,  1.0367e+00, -1.3318e+00, -5.9293e-01,\n",
       "         1.5719e+00, -1.6458e+00,  4.1092e-01, -1.6035e+00,  8.7556e-02,\n",
       "        -3.1622e-01,  6.3328e-01,  4.7031e-01, -1.8899e+00, -6.1608e-01,\n",
       "        -4.7564e-01,  3.0132e-01, -6.8131e-01,  8.5480e-01,  6.7310e-02,\n",
       "         6.5187e-01, -2.6407e+00,  6.8429e-01,  9.1604e-01, -3.3671e-01,\n",
       "        -1.0715e-01,  1.1834e+00, -1.1082e+00,  3.8607e-01, -4.1875e-02,\n",
       "         7.9574e-01, -7.3058e-01,  1.2287e+00,  1.1001e+00, -1.5659e+00,\n",
       "        -1.0764e+00,  8.1070e-01, -8.1547e-01,  1.5400e+00, -5.6903e-01,\n",
       "         1.4833e-01,  5.6806e-01,  7.4400e-01,  3.0491e-01, -4.8282e-01,\n",
       "         9.3094e-01,  1.7509e+00,  1.3941e+00,  1.9753e+00, -1.0095e+00,\n",
       "         1.0933e+00,  7.3636e-01,  2.1361e+00, -1.3852e+00, -6.8936e-01,\n",
       "         6.8198e-03, -2.7570e+00,  1.3798e-01,  9.4985e-01,  2.0211e+00,\n",
       "        -3.5411e-01,  4.1297e-01, -5.2028e-01,  2.5268e-01,  5.8260e-01,\n",
       "        -7.4234e-01, -6.5104e-02, -6.8509e-01, -6.2830e-01, -1.6252e+00,\n",
       "        -3.5222e-01, -1.8713e-01,  4.0533e-01, -5.8343e-01, -3.1145e-01,\n",
       "         2.9074e-01, -3.4207e-01, -1.3754e+00,  7.4406e-02, -3.4122e-01,\n",
       "         1.5202e+00,  1.1335e+00, -1.4067e+00,  9.2807e-01, -9.4195e-01,\n",
       "        -1.5061e+00,  4.8189e-01,  9.4087e-01,  7.6101e-01, -2.2155e-01,\n",
       "        -1.1905e+00, -7.8772e-01,  1.5918e+00, -9.6627e-01,  7.6247e-02,\n",
       "         1.4960e+00, -3.1839e-01,  2.8460e+00,  3.1314e-01,  1.1141e+00,\n",
       "        -7.2453e-01, -1.5616e-01, -8.1504e-01,  1.1646e+00, -4.0074e-01,\n",
       "        -7.4532e-01, -1.8344e+00,  2.0059e+00, -2.3779e-01, -1.7241e+00,\n",
       "         1.0913e+00, -5.4236e-01,  3.7467e-01, -4.2269e-01, -3.1174e-01,\n",
       "        -5.1329e-02,  6.5822e-01,  1.5283e+00,  3.2422e-01, -1.6448e+00,\n",
       "         3.0819e-01, -1.5747e+00,  1.0866e+00,  4.6278e-01, -1.1468e+00,\n",
       "         1.8858e-01,  1.4242e+00, -8.9541e-01,  2.5728e-01, -9.2797e-01,\n",
       "         1.3246e+00,  7.3366e-01, -2.9155e-01,  1.0502e+00, -5.8290e-01,\n",
       "        -3.1994e-01, -1.0297e+00,  6.0298e-01, -6.1556e-01, -1.5168e+00,\n",
       "         7.1529e-01, -1.0695e+00, -9.2162e-01, -8.1979e-02, -7.5926e-01,\n",
       "        -1.0128e+00,  1.9699e+00,  4.8464e-01,  2.2973e-01, -9.3368e-01,\n",
       "         1.6573e-01, -1.1144e+00, -1.9447e+00, -2.8647e-01, -6.3571e-01,\n",
       "        -4.1662e-02, -5.5261e-01,  2.3953e+00, -3.5002e-02, -1.9663e+00,\n",
       "         7.6960e-01, -1.3650e+00,  2.6410e-01, -1.2225e+00,  5.4012e-01,\n",
       "         3.6182e-01, -2.0359e+00, -9.4016e-01,  7.6703e-01,  9.2194e-02,\n",
       "        -1.3378e-01,  6.6056e-02, -1.2026e+00,  1.3147e+00, -3.9377e-01,\n",
       "         1.8490e+00, -1.3111e+00,  2.6266e+00,  1.1910e-01, -7.9855e-01,\n",
       "        -1.1544e+00,  1.8778e-01, -6.2948e-01,  2.3960e-01,  6.8357e-01,\n",
       "        -1.1588e+00,  5.4671e-01,  7.9702e-01, -2.2593e+00,  1.1480e+00,\n",
       "         3.1484e-01,  2.0245e-01, -3.9837e-01,  6.4098e-01, -1.1987e+00,\n",
       "         1.0091e+00,  3.0190e-01, -5.8371e-01,  9.5493e-01,  1.3503e+00,\n",
       "        -4.6970e-01, -8.2533e-02, -1.4875e+00, -1.6334e+00,  3.6369e-01,\n",
       "         1.1669e+00, -4.2262e-02, -3.7936e-01,  6.6207e-01, -1.3896e+00,\n",
       "        -5.8237e-01, -2.7913e-01,  2.3429e+00,  1.7231e-01, -1.8824e+00,\n",
       "        -1.2612e+00,  8.7422e-01,  4.4380e-01])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc[0][0][56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.3511e-01,  9.7620e-01,  1.6881e-01, -1.6746e-01,  1.7394e+00,\n",
       "         4.1351e-01, -9.3527e-01, -1.4215e+00,  2.0797e-01,  4.3854e-01,\n",
       "         1.1743e+00,  2.0035e-02, -7.7253e-02,  3.6862e-02,  9.1399e-01,\n",
       "         4.8420e-01,  2.5521e-01,  2.4548e-01,  8.0901e-01, -5.5270e-01,\n",
       "        -1.2932e+00,  1.1728e+00,  1.3104e+00, -6.4525e-04,  1.2407e+00,\n",
       "         1.5431e+00,  6.0450e-01, -3.5695e-01,  3.2148e-01,  8.8375e-01,\n",
       "         5.8738e-01,  3.5735e-01,  5.8643e-01,  7.0485e-01,  1.5429e+00,\n",
       "        -1.3873e+00, -3.6883e-01, -1.8330e+00,  9.5843e-01,  4.2259e-01,\n",
       "         8.2388e-01,  1.3947e-01, -1.2895e+00,  5.2436e-01, -1.4023e-01,\n",
       "        -1.1777e+00,  1.1356e+00,  1.7126e+00, -2.7257e-01,  6.3837e-01,\n",
       "         1.4524e+00, -4.6587e-01, -8.1866e-01,  7.1335e-01,  1.0268e+00,\n",
       "        -4.3458e-01,  1.1199e+00, -1.0428e+00, -1.6140e+00, -2.9325e-01,\n",
       "         1.3460e+00,  3.6872e-01, -4.7892e-01, -5.7957e-01, -5.4396e-01,\n",
       "         1.0506e+00, -4.6336e-01,  3.5428e-01,  6.5351e-01,  6.2968e-02,\n",
       "        -1.3690e+00, -2.2590e-01, -1.2106e+00, -2.6453e-02,  1.0542e+00,\n",
       "        -9.6603e-01, -5.7230e-01, -2.2579e+00,  1.2543e+00, -5.2673e-01,\n",
       "        -9.0538e-02,  2.0268e-01,  4.9588e-02,  2.9077e-01,  4.1541e-01,\n",
       "        -9.3501e-01, -5.5406e-01,  9.4447e-01, -4.3393e-01, -2.3091e+00,\n",
       "         6.8890e-01, -8.0090e-01, -1.6953e+00,  1.5386e+00,  1.3116e-01,\n",
       "        -1.0377e+00, -9.1652e-01,  6.6124e-01,  9.2871e-01,  1.5769e+00,\n",
       "        -5.2185e-01, -1.6372e+00,  8.3037e-01, -8.4407e-02, -3.4898e-02,\n",
       "         3.2124e-01, -5.4648e-01, -1.6684e-01, -1.1038e+00,  6.8192e-01,\n",
       "        -5.7394e-01, -1.0893e+00,  8.8244e-01,  4.9798e-01, -5.7236e-01,\n",
       "        -1.1922e+00,  1.2031e+00, -1.9485e-02,  6.0815e-01, -3.0739e-01,\n",
       "         1.4541e+00,  1.4966e+00,  1.7167e+00,  1.3112e-01,  2.3045e+00,\n",
       "         1.7322e-02,  4.3987e-01, -1.3425e+00, -1.2719e+00,  6.5115e-01,\n",
       "        -7.2413e-01, -5.0854e-01,  6.5181e-01, -1.5087e+00, -5.4143e-01,\n",
       "        -4.7296e-01,  2.3797e-01,  3.3954e-01, -5.6377e-02,  1.1960e+00,\n",
       "        -4.6339e-01, -5.2514e-01,  2.3483e+00,  4.8198e-01,  2.3551e-01,\n",
       "        -1.6526e+00, -4.8740e-01,  2.0361e-01,  4.0265e-01,  5.0238e-01,\n",
       "        -1.1678e+00,  1.5085e-01,  8.3159e-01,  3.9716e-02,  1.8216e-01,\n",
       "         2.8698e-01,  4.0904e-01,  7.2202e-02,  9.3029e-01,  8.8888e-01,\n",
       "        -5.8078e-01,  2.1720e+00, -1.1231e+00,  7.4745e-02, -9.8731e-01,\n",
       "         1.6537e-01, -1.1415e+00, -6.7616e-01,  1.3695e+00,  2.7505e-01,\n",
       "         1.0159e+00, -1.1902e-01,  1.2910e+00, -8.5985e-01,  5.4959e-01,\n",
       "         5.2088e-01,  1.6996e+00,  4.8234e-02, -1.2219e+00,  3.2850e-01,\n",
       "         5.4938e-01, -7.7985e-01,  4.5738e-01, -1.2702e+00, -1.7977e+00,\n",
       "        -9.1944e-01, -7.1736e-01, -1.5421e-01, -8.6090e-01,  1.4554e+00,\n",
       "         2.4220e-02,  5.5901e-01,  1.0348e+00,  1.2157e+00, -9.1099e-01,\n",
       "         1.0254e+00,  1.1791e+00,  1.0953e-01,  3.0346e-01,  2.9272e-02,\n",
       "        -1.4839e-01, -1.4900e+00, -7.5050e-01,  9.6425e-01,  1.2160e+00,\n",
       "        -1.1277e+00, -2.2517e-01, -4.8114e-01, -5.9507e-02, -1.3303e+00,\n",
       "         6.6954e-01,  1.3588e-01, -4.7997e-01, -1.8483e+00,  1.9332e+00,\n",
       "        -2.8176e-01,  1.1858e+00,  1.2924e+00,  1.7637e-01,  2.2592e-02,\n",
       "        -1.5843e+00,  2.1634e-01,  2.5279e+00,  8.8079e-01,  1.1911e+00,\n",
       "         7.5044e-03, -7.4788e-01, -4.3669e-01,  3.2850e-02, -8.7596e-01,\n",
       "        -5.4597e-01,  3.4719e-01, -3.2931e-01, -6.5693e-01, -9.7433e-01,\n",
       "         4.5297e-01, -1.1126e+00,  2.3910e-02, -1.4293e-01, -6.7876e-02,\n",
       "         2.1529e-01,  2.6805e+00, -2.1778e-01,  1.3229e+00, -6.0479e-01,\n",
       "        -3.3094e-01, -1.3183e+00,  1.2592e+00,  1.3878e-01,  1.6260e-01,\n",
       "         1.5067e+00, -5.5445e-01, -1.4409e-03, -8.9676e-01,  9.2665e-01,\n",
       "         9.5813e-01, -2.8648e-01,  3.3444e-01,  3.1523e-01, -2.9885e-01,\n",
       "         2.4959e-01, -1.0412e+00,  8.3721e-01, -3.9839e-01, -2.5304e+00,\n",
       "        -2.7413e+00,  2.2022e+00, -3.0139e-01, -1.3940e-01, -9.4577e-01,\n",
       "         8.3161e-01,  2.9120e-01,  4.8202e-01, -9.6298e-01, -2.0070e-01,\n",
       "         1.4098e-01,  2.4565e-01, -9.9318e-01,  8.9100e-01,  4.0558e-01,\n",
       "         1.5716e+00,  1.8254e+00,  1.3738e+00, -1.7241e+00,  1.3577e+00,\n",
       "         6.7304e-01,  1.8047e-01,  2.3524e+00, -2.1872e+00, -5.7566e-01,\n",
       "         3.8654e-01,  7.8990e-01,  4.3575e-01,  7.1891e-01,  7.4674e-01,\n",
       "        -2.0665e+00, -6.0568e-01, -7.9180e-01,  1.0112e+00,  1.2836e-01,\n",
       "         2.9644e-01, -8.0872e-01, -1.0214e+00,  4.3277e-01,  1.3714e+00,\n",
       "        -2.1780e-01, -1.5230e+00, -1.4752e-01, -3.9799e-02, -1.0260e+00,\n",
       "         6.7473e-01,  1.1814e-01,  1.8624e+00, -4.6613e-01, -1.9917e+00,\n",
       "        -1.0907e+00, -7.8261e-01,  2.3056e-02,  6.9938e-01, -2.9658e-01,\n",
       "         4.3695e-01, -1.3792e+00, -2.1699e+00,  9.9669e-01, -1.0600e+00,\n",
       "         2.5720e-01, -1.3864e+00,  4.1876e-01, -7.5989e-01,  7.9851e-01,\n",
       "        -1.3259e+00,  3.1733e-01,  1.1678e+00, -1.1592e+00, -2.2641e-01,\n",
       "         1.5025e+00,  5.0116e-01,  3.1044e-01, -1.1888e-01, -5.2474e-01,\n",
       "         9.6165e-01,  1.4004e+00, -1.7822e+00,  1.6519e+00, -1.1204e-01,\n",
       "         2.3621e-01, -4.0488e-01, -1.0306e+00, -1.3146e+00, -1.5946e-02,\n",
       "        -6.6889e-01,  4.6974e-01,  3.2733e-01,  8.6028e-01,  1.7692e+00,\n",
       "        -9.7752e-01,  7.8333e-02,  2.1355e-02,  5.4694e-01,  5.2398e-01,\n",
       "         1.5231e+00, -1.6599e+00,  1.6481e+00,  3.9555e-01,  5.1788e-01,\n",
       "        -7.1079e-01, -2.4969e-01,  1.0275e-01, -1.4628e-01,  5.4981e-01,\n",
       "         9.9401e-01,  7.8861e-01, -1.7595e+00,  1.6572e+00, -3.8584e-01,\n",
       "        -7.1097e-01,  1.1342e+00, -1.4395e+00, -2.4505e+00, -5.8736e-01,\n",
       "        -8.1375e-01, -7.1548e-01, -1.9069e+00,  5.1434e-01, -7.1938e-01,\n",
       "        -4.3634e-01,  1.1494e+00,  7.6791e-01, -1.4103e+00,  4.8578e-01,\n",
       "         1.8421e+00, -1.0869e+00, -8.1573e-01,  2.6974e-01, -1.0719e+00,\n",
       "         7.2789e-01,  5.2114e-02, -3.1222e-01,  2.1940e-01, -1.3528e+00,\n",
       "        -1.5087e+00, -3.8668e-01,  1.5928e+00, -4.2655e-01,  1.2761e-01,\n",
       "        -3.7235e-01,  5.3314e-02, -8.6720e-01,  7.6439e-01,  1.0400e+00,\n",
       "        -2.6556e+00, -1.8044e+00,  1.4020e+00, -5.5435e-01, -5.3965e-01,\n",
       "         5.9286e-01,  9.1392e-01, -1.8829e+00,  3.2017e-02, -6.0062e-02,\n",
       "        -1.7549e+00,  1.7836e+00, -6.9289e-01, -3.8734e-01, -2.0600e-01,\n",
       "        -3.7298e-01, -1.4654e+00, -1.3984e+00, -6.1682e-01,  4.3530e-01,\n",
       "         5.6003e-02,  4.8527e-01, -3.9313e-01,  1.0933e+00,  3.9605e-01,\n",
       "        -5.7945e-01,  5.5077e-01, -4.2004e-01,  5.5488e-01,  3.6733e-01,\n",
       "        -3.7998e-01,  1.3436e+00, -6.1640e-01,  1.3007e+00, -2.1102e+00,\n",
       "        -2.1456e+00,  1.6097e+00, -3.0005e-01,  1.0346e-01,  6.7397e-01,\n",
       "         7.4808e-01,  2.5568e-01, -6.0264e-01,  8.1669e-01, -2.9017e-01,\n",
       "         1.0829e+00, -8.0867e-01,  1.4108e+00,  1.1348e+00, -6.8627e-01,\n",
       "        -1.5188e+00, -2.1007e+00, -1.7566e+00,  7.8679e-02, -8.3042e-01,\n",
       "         8.9158e-01, -1.8284e+00,  3.4155e-01,  1.0335e+00, -2.6425e-01,\n",
       "        -8.2705e-01, -3.2363e-01,  6.5825e-01,  2.2419e+00,  8.4875e-01,\n",
       "         3.1017e-01, -2.0952e-01,  8.5553e-01, -7.4556e-01, -7.8820e-01,\n",
       "        -4.4568e-01, -2.8621e-01,  9.5839e-01,  2.4672e-02,  7.7408e-01,\n",
       "         1.0165e+00, -9.9035e-01,  1.4417e+00,  7.2206e-01,  5.9476e-01,\n",
       "        -2.4918e+00, -1.0548e+00, -1.1570e+00,  8.0023e-01,  1.8034e+00,\n",
       "         1.3144e-02, -1.6282e+00, -7.9863e-01,  7.6878e-01,  6.6111e-01,\n",
       "        -1.4772e+00, -2.1782e-01, -5.7956e-01, -5.8913e-01, -1.7890e+00,\n",
       "        -1.6621e+00, -6.8000e-01,  2.5313e-01,  1.1868e+00, -8.6104e-01,\n",
       "        -4.0674e-01,  2.3321e-01,  6.2125e-01, -7.4106e-01,  1.1394e+00,\n",
       "        -6.3486e-01,  1.0494e+00, -1.9442e-01,  1.7927e-01, -1.4645e+00,\n",
       "         1.3466e+00, -5.8498e-01,  7.2823e-01, -1.6416e-01,  1.0364e+00,\n",
       "         4.8812e-01, -7.1359e-01, -7.8507e-01,  1.7888e-01,  3.2803e-01,\n",
       "         5.1316e-01,  4.7292e-01, -1.0690e-01, -6.1800e-01, -6.7142e-01,\n",
       "        -2.3784e-01,  1.0122e-01, -3.0957e-01,  2.8735e-01, -1.1620e+00,\n",
       "         8.0470e-01,  1.0095e+00, -1.2705e-01,  2.6127e+00, -4.7663e-01,\n",
       "        -1.4395e+00, -1.0964e+00,  1.5174e+00,  7.5408e-01,  1.3292e+00,\n",
       "         2.9985e-02,  1.6877e+00,  4.0544e-01, -1.6609e-01, -1.1541e+00,\n",
       "         4.8728e-01,  1.3012e+00, -1.0233e+00, -1.1065e+00,  7.2709e-02,\n",
       "        -1.6188e-01,  5.9925e-01,  8.8244e-01,  2.8885e-01,  8.3188e-01,\n",
       "        -8.0041e-01, -4.7161e-01, -7.7252e-01, -8.9252e-05,  3.8591e-02,\n",
       "        -1.3622e+00, -1.8230e+00,  2.2723e+00,  6.0073e-01,  1.4384e-01,\n",
       "        -6.8048e-01,  2.9737e-01,  6.6678e-01,  6.5281e-01,  1.6470e+00,\n",
       "         1.6648e+00,  3.6351e-01,  5.8553e-01,  5.5260e-01, -1.7845e+00,\n",
       "        -9.7775e-01,  7.2413e-01,  1.7927e-01,  1.1655e+00,  4.9804e-01,\n",
       "         4.5221e-02,  3.2031e-01, -3.8010e-01, -2.3624e-01, -4.5065e-01,\n",
       "         1.0245e+00, -1.0444e-01, -1.2824e+00,  5.6821e-01, -1.2447e+00,\n",
       "         7.9068e-01, -5.4556e-01,  1.9162e+00, -7.6402e-01,  1.5739e-01,\n",
       "        -7.9721e-01, -1.0614e+00, -7.9107e-01, -7.2023e-01,  2.2300e+00,\n",
       "        -5.6571e-02,  1.4058e+00, -1.6012e-01,  7.0754e-01,  1.6189e+00,\n",
       "        -8.2563e-02, -8.8572e-01,  2.0748e-01, -1.7195e-01,  6.9745e-01,\n",
       "         3.0035e-01, -1.3837e+00, -5.4571e-03,  2.2175e+00, -9.4689e-01,\n",
       "         3.4765e-01,  1.7659e+00, -4.4483e-01, -1.1199e+00,  6.4684e-01,\n",
       "         1.8783e+00,  8.6839e-01,  9.3184e-02,  1.2870e-01, -6.1574e-03,\n",
       "         4.4599e-01, -1.0426e+00,  5.5351e-01,  2.7724e-01,  1.0308e-01,\n",
       "        -8.4081e-01,  3.1532e-01,  4.5059e-01, -1.2164e+00,  1.0248e+00,\n",
       "         1.8798e+00, -6.6504e-01,  2.4362e+00,  6.5971e-01,  9.1378e-01,\n",
       "        -1.5600e-01, -3.4146e-01, -7.4693e-01,  1.1824e+00,  8.0941e-01,\n",
       "        -3.1859e-01, -2.1739e+00,  3.9464e-01,  2.1352e-01, -1.6363e+00,\n",
       "         1.1930e+00,  7.9877e-01, -1.0331e+00,  1.3122e+00, -7.8089e-01,\n",
       "        -1.3254e+00,  9.0289e-01,  2.4506e-01,  3.1358e-01, -1.8694e+00,\n",
       "         4.7115e-02, -9.5379e-01,  7.7701e-01,  2.4801e+00, -1.2289e+00,\n",
       "        -3.6144e-01,  5.6480e-01, -5.4573e-01,  7.0239e-01, -7.9297e-01,\n",
       "         7.3679e-01, -1.5879e-01, -6.2918e-01,  9.6672e-01, -4.8238e-01,\n",
       "        -1.1930e+00, -9.7266e-01,  1.6733e+00, -4.5458e-01, -9.9384e-02,\n",
       "         5.6331e-01, -1.0852e+00, -2.7477e+00, -9.8774e-01,  1.6442e+00,\n",
       "        -2.6658e-01,  1.0511e+00,  1.3942e+00, -3.5527e-01, -1.3071e+00,\n",
       "        -2.4660e+00, -1.6296e+00, -3.3633e+00, -5.3112e-01, -1.5228e+00,\n",
       "        -1.2516e+00,  7.7250e-01,  1.0288e+00,  8.3131e-01, -1.5005e+00,\n",
       "         1.0300e+00, -1.8296e+00,  2.0718e-01, -1.3894e+00,  8.2120e-02,\n",
       "         6.1571e-01, -9.7023e-01, -1.2329e-01, -5.5785e-01,  4.5161e-01,\n",
       "         2.3201e-01,  1.4226e+00, -3.8122e-01, -8.7378e-01, -4.0337e-01,\n",
       "         1.0802e+00, -6.7114e-01,  2.8647e-01, -9.7476e-01, -1.1444e+00,\n",
       "        -3.7048e-01, -1.0216e+00,  6.7199e-02,  5.8668e-01, -4.1619e-01,\n",
       "         9.1606e-02,  2.6999e-01,  2.1985e+00, -3.3242e-02,  9.5526e-01,\n",
       "        -2.4758e+00, -1.3442e+00, -7.0504e-01, -2.1365e+00, -1.2680e-01,\n",
       "         1.1435e-01, -2.1499e-01,  3.7761e-01, -5.1669e-01,  1.3883e-01,\n",
       "         9.4384e-01,  9.5582e-01, -2.3151e-01, -4.7331e-01, -1.2265e-01,\n",
       "         1.0054e+00,  4.0460e-01,  3.7334e-01,  9.8891e-01, -3.7639e-01,\n",
       "        -1.0331e+00,  1.6265e+00, -1.0087e+00, -2.2839e-01,  1.9930e-01,\n",
       "        -9.2642e-01,  2.8869e-01, -1.7719e-01])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc[0][0][25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 57, 768])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.5098e-01, -7.9196e-01, -2.1698e-01, -3.6640e-01,  1.5158e-01,\n",
       "         -1.5165e-01, -1.0893e-01, -1.3903e-01, -5.2109e-01, -6.1910e-01,\n",
       "         -8.2561e-01,  4.2614e-01,  6.9227e-01,  2.9236e-01,  1.8154e-01,\n",
       "          7.4684e-02, -6.9392e-01, -6.1038e-01,  4.2819e-01, -3.6064e-01,\n",
       "          7.5615e-01, -1.5883e-01,  8.2739e-01,  7.6967e-02, -5.1764e-01,\n",
       "         -5.2033e-01,  5.8470e-01,  3.2987e-01,  5.3080e-02, -6.0996e-01,\n",
       "          7.0961e-02, -2.9388e-01,  5.0768e-01, -5.1933e-01, -7.2867e-01,\n",
       "         -9.4709e-02, -6.2928e-01, -5.6185e-01,  1.2785e-01, -5.2781e-01,\n",
       "          8.2436e-01,  8.9368e-01,  7.9929e-01, -3.8593e-01, -2.2878e-01,\n",
       "          5.1985e-01, -1.5469e-01, -5.7444e-01, -5.4170e-01, -4.2583e-01,\n",
       "         -6.7562e-01,  5.3583e-01, -6.9473e-01, -7.5306e-02, -3.6913e-01,\n",
       "          2.6101e-01,  4.1753e-01,  1.9410e-01, -1.9219e-01,  3.2181e-01,\n",
       "         -3.3338e-01,  2.8422e-01, -1.8168e-02, -6.1339e-01,  3.0172e-01,\n",
       "         -7.9172e-02, -3.7844e-01,  4.8809e-02, -4.0456e-01, -2.4728e-01,\n",
       "         -6.1096e-01,  9.0753e-01,  5.4195e-01,  7.1230e-01, -3.3502e-01,\n",
       "         -2.3000e-01,  3.7615e-01,  4.7040e-01, -1.5759e-01, -2.2705e-01,\n",
       "          8.8663e-01, -6.9718e-01, -3.7398e-01,  3.5992e-01,  5.8475e-01,\n",
       "          3.6429e-01, -3.3474e-01,  1.9060e-01, -2.5432e-01, -1.3011e-01,\n",
       "          2.9987e-01, -4.9210e-01,  6.4743e-01, -5.3831e-01, -1.1968e-01,\n",
       "         -4.8547e-01, -4.2854e-01, -6.4268e-01, -4.5875e-01,  1.9033e-01,\n",
       "         -1.3350e-03,  5.4591e-01,  4.9725e-02,  6.7051e-02, -4.0489e-01,\n",
       "          2.2194e-02,  3.6957e-02,  3.7248e-01, -6.9220e-01,  6.8714e-01,\n",
       "         -4.3666e-01,  8.9837e-01,  5.5008e-01, -1.4887e-01, -1.7441e-01,\n",
       "         -8.4402e-01,  2.5469e-01, -2.8564e-04, -4.7952e-01, -2.2976e-01,\n",
       "         -8.6973e-01,  2.8407e-01,  6.8879e-01, -2.7601e-01, -3.6474e-01,\n",
       "          1.9948e-01, -3.5726e-01,  3.4889e-02, -7.7024e-03, -1.6495e-01,\n",
       "         -5.4091e-01,  4.4815e-01, -4.7388e-02,  2.3572e-01,  2.0052e-01,\n",
       "         -3.1437e-01, -5.4782e-01,  7.1258e-01, -7.8987e-01,  5.6568e-01,\n",
       "          3.0758e-01,  1.2519e-01,  4.3610e-01, -7.7203e-01,  4.5649e-01,\n",
       "          9.1293e-01,  6.2868e-01, -2.3879e-01, -1.0734e-01, -2.1725e-01,\n",
       "          4.3750e-01, -5.6037e-01, -5.5928e-01,  7.8506e-01,  1.5227e-01,\n",
       "          2.1948e-01, -6.0718e-01,  4.0384e-01,  2.3270e-01, -6.1636e-01,\n",
       "          1.5734e-01,  1.8240e-01,  3.8397e-01, -4.5162e-01, -3.5871e-01,\n",
       "         -3.5716e-01,  2.7109e-01, -4.0267e-01,  4.3118e-01, -6.2521e-01,\n",
       "          6.8180e-01,  3.9506e-01, -3.6915e-01,  8.9484e-02,  1.3949e-01,\n",
       "          5.1115e-02,  6.8244e-01,  5.8933e-01,  1.0529e-01, -3.4922e-01,\n",
       "         -4.3984e-01,  8.4169e-01, -4.4078e-01, -3.2352e-01, -4.3346e-01,\n",
       "         -3.2477e-02,  4.3369e-01, -1.8710e-01, -6.4703e-02, -9.8384e-02,\n",
       "         -5.4549e-01,  7.0178e-02, -5.7347e-02, -2.6738e-01,  1.7262e-01,\n",
       "         -2.0519e-01, -3.6862e-01, -3.0231e-01, -3.2130e-01, -4.3925e-01,\n",
       "         -4.5637e-01, -2.9794e-01, -5.5802e-01, -1.9393e-01, -2.9313e-01,\n",
       "         -1.8532e-01,  1.2715e-01, -7.0972e-01, -1.9119e-01, -2.8819e-03,\n",
       "         -8.5882e-02,  1.1690e-01, -4.1385e-01, -5.6492e-01, -8.4818e-01,\n",
       "          4.3850e-02, -1.5093e-01, -7.7832e-01, -6.0856e-01,  3.3055e-01,\n",
       "         -6.3055e-01, -2.3238e-01, -6.2837e-01,  1.8323e-01, -4.3737e-01,\n",
       "         -4.1048e-01, -4.0798e-01, -1.1985e-01,  6.6828e-01,  3.8634e-01,\n",
       "         -4.4049e-01,  2.3730e-01,  5.5947e-01,  8.2090e-01,  3.6245e-01,\n",
       "          3.5925e-01, -5.5825e-01,  9.4039e-02,  1.2942e-01,  3.5832e-01,\n",
       "          4.9910e-01,  5.7864e-01, -2.7018e-01, -7.0465e-01, -5.5802e-01,\n",
       "         -3.8323e-01,  3.5038e-01, -2.0026e-01, -5.7911e-01,  1.8491e-01,\n",
       "         -2.6050e-03, -4.3303e-01,  1.0563e-01, -4.8342e-02, -4.5493e-01,\n",
       "          4.6044e-01,  6.6182e-01,  3.0930e-01, -7.2389e-01,  2.4226e-01,\n",
       "         -4.9942e-02, -1.5566e-01, -3.8945e-01, -5.7491e-01,  3.9966e-01,\n",
       "          3.0629e-01,  3.8343e-01, -4.1176e-01, -6.0760e-02, -5.5877e-01,\n",
       "         -1.7933e-01,  4.1627e-02,  7.5110e-01, -4.8608e-01, -5.8269e-01,\n",
       "          1.8489e-01,  5.7009e-01,  1.1991e-01, -7.7456e-01, -4.2906e-01,\n",
       "          8.0675e-02,  1.0144e-01, -3.4129e-01,  1.9680e-01, -1.7266e-01,\n",
       "          9.8654e-02, -9.2284e-02, -2.1743e-01, -3.6409e-01, -1.0177e-01,\n",
       "         -4.6248e-01, -4.3203e-01,  3.2323e-01, -2.6742e-01,  6.9620e-01,\n",
       "          7.4607e-01, -2.2525e-01, -7.6887e-02, -5.5952e-02,  5.2897e-01,\n",
       "          4.1456e-01,  5.8856e-01,  2.7749e-01,  2.5398e-02, -1.5924e-01,\n",
       "          4.7486e-01, -8.3818e-02, -4.0744e-01, -5.9377e-01, -3.8218e-02,\n",
       "         -3.5377e-01, -2.9216e-01,  4.7541e-01, -4.4247e-01,  2.0902e-01,\n",
       "          2.5784e-01, -4.8433e-01,  2.6353e-02, -2.0941e-01, -8.5506e-01,\n",
       "          5.8952e-01,  1.2598e-01, -1.6827e-01,  3.0171e-01, -1.7876e-01,\n",
       "          9.6338e-02,  6.1957e-01, -2.5432e-01,  7.1791e-01, -6.8000e-01,\n",
       "         -5.0073e-01, -2.2851e-01, -3.6789e-01, -5.5163e-01, -8.4586e-01,\n",
       "          5.9001e-01, -2.3382e-02,  1.7963e-01, -9.8437e-02,  7.7401e-01,\n",
       "          2.1529e-01, -1.6289e-01, -6.8845e-02, -2.8173e-01, -4.4648e-01,\n",
       "          8.6609e-01, -3.6337e-02,  6.0904e-01, -4.6477e-01,  1.2102e-01,\n",
       "         -3.7115e-01, -4.7791e-01,  5.1706e-01,  3.8695e-01,  1.9414e-01,\n",
       "          4.4117e-01,  1.7452e-01, -5.1833e-01,  4.1244e-01,  4.2773e-01,\n",
       "          4.7535e-01,  1.1045e-01,  7.2124e-01,  7.5510e-01, -2.5036e-01,\n",
       "          2.4206e-01, -4.6992e-01, -8.7217e-01, -1.8835e-01,  5.2210e-01,\n",
       "         -1.0989e-01,  7.3713e-01, -6.1768e-02, -3.4386e-01,  1.7869e-01,\n",
       "         -3.9527e-01, -5.5508e-01,  8.0637e-01,  7.6349e-01, -8.0182e-01,\n",
       "          4.1115e-01,  3.7431e-01, -1.0505e-01,  4.7950e-01,  8.3477e-01,\n",
       "         -1.0277e-01,  3.9554e-01,  7.1016e-01, -6.1716e-01,  4.4793e-01,\n",
       "         -3.7723e-01,  5.7090e-01,  1.9876e-01,  2.8916e-01, -4.2085e-01,\n",
       "         -6.9739e-01, -4.8368e-01, -3.2247e-01, -4.5183e-01,  4.5750e-01,\n",
       "         -5.7021e-01,  1.7998e-01,  5.8314e-01, -1.6072e-01, -6.7173e-01,\n",
       "         -2.8382e-01,  7.9528e-01, -7.9265e-01, -5.8912e-01,  2.5393e-01,\n",
       "         -1.9077e-01, -6.0997e-01,  5.0068e-01, -6.1262e-01,  7.0891e-01,\n",
       "          3.4854e-01, -3.2512e-02, -1.8983e-01,  5.4062e-01,  4.8206e-01,\n",
       "         -5.2029e-01, -3.5772e-01,  1.0763e-01,  3.5351e-01, -5.0234e-02,\n",
       "          7.3253e-01, -2.3443e-01, -2.4486e-02,  5.3277e-01,  2.7049e-01,\n",
       "          6.6168e-01, -6.0431e-01, -5.8691e-01,  5.5588e-01, -1.7614e-01,\n",
       "          1.3820e-01, -4.1915e-01, -2.9325e-01,  1.9768e-01,  2.7523e-01,\n",
       "         -5.6706e-01,  3.5375e-01, -2.2460e-01,  5.4318e-01,  2.1654e-01,\n",
       "          7.0419e-01,  4.1587e-01,  2.7984e-01,  1.4743e-01,  4.0999e-01,\n",
       "          5.5393e-01, -6.3947e-01,  2.6227e-01,  9.2530e-02, -3.4299e-01,\n",
       "          5.0768e-01,  6.1484e-01, -7.8318e-01, -1.7498e-01, -2.2992e-01,\n",
       "          4.0826e-01,  5.6007e-01, -1.7187e-01, -2.2483e-01, -6.7077e-02,\n",
       "          2.2556e-01, -7.8773e-02,  1.3923e-01, -2.2293e-01, -4.8194e-01,\n",
       "          7.3855e-01,  5.8134e-01, -3.9977e-01,  3.9044e-01, -7.4894e-02,\n",
       "          8.2436e-02, -3.8745e-01, -8.5639e-02,  8.5372e-02, -6.5443e-01,\n",
       "          3.7176e-01,  2.7464e-01,  4.8376e-01, -3.7761e-01,  5.6493e-01,\n",
       "          7.5447e-01,  8.3260e-02, -3.2772e-01,  3.6153e-01,  5.6866e-01,\n",
       "          1.4975e-02,  7.2413e-01,  1.1060e-01, -4.4575e-01, -4.6223e-01,\n",
       "         -8.0331e-01,  2.4476e-01, -2.1366e-01, -1.7711e-01,  2.3230e-01,\n",
       "          8.0701e-01, -4.4042e-01,  3.5011e-01,  2.9670e-01, -5.0822e-01,\n",
       "         -1.0955e-01, -4.9650e-01, -2.3165e-02, -4.0694e-01, -1.4953e-01,\n",
       "          1.1821e-01,  2.9479e-01, -8.8049e-01, -2.8683e-01,  3.0059e-01,\n",
       "         -3.5271e-01, -3.1935e-01,  6.1359e-02,  4.2632e-01, -5.6460e-01,\n",
       "         -7.0817e-01,  6.0859e-01, -2.3666e-01,  7.3522e-01, -2.4866e-01,\n",
       "         -2.9422e-02, -9.8998e-02,  2.0540e-01,  4.9871e-01, -1.1235e-01,\n",
       "         -4.3534e-01,  5.9235e-01, -3.2061e-01, -4.3555e-02,  4.4142e-01,\n",
       "         -7.3175e-01,  5.4400e-01, -5.8719e-01, -4.6394e-01,  2.5077e-03,\n",
       "         -9.0473e-01,  1.8672e-01, -3.0655e-01, -1.5455e-01,  7.1617e-02,\n",
       "          6.6463e-01, -6.0649e-01, -2.7895e-01, -7.7131e-01,  1.1241e-01,\n",
       "          1.3028e-01, -4.9558e-01, -8.9176e-01, -3.8773e-02,  3.8777e-01,\n",
       "         -7.1583e-01,  3.6560e-01, -2.7658e-01, -5.2963e-01, -8.4277e-02,\n",
       "         -2.1103e-02,  3.1964e-01,  5.1486e-02,  8.4422e-01,  4.3060e-01,\n",
       "          6.0751e-01,  5.8525e-01,  1.2979e-01, -7.5176e-01,  6.4309e-02,\n",
       "         -6.4046e-01,  1.6601e-01, -1.6850e-01,  3.3085e-01,  5.2002e-01,\n",
       "          3.7140e-01, -2.5078e-01, -6.5751e-01,  1.2499e-01,  3.1016e-01,\n",
       "         -7.6820e-01,  4.4278e-02,  6.8665e-02, -4.7163e-01, -1.5290e-01,\n",
       "          3.3138e-01,  6.6039e-01,  7.2574e-01,  3.4528e-03, -8.7543e-01,\n",
       "         -5.3682e-01, -3.6858e-01,  8.8976e-02, -1.7942e-01, -5.0986e-01,\n",
       "          2.5972e-01,  1.2888e-01, -6.0832e-02,  8.0853e-01,  3.4906e-01,\n",
       "          3.7015e-01, -6.5724e-01,  7.1556e-01, -4.7973e-01, -1.2094e-01,\n",
       "         -4.5193e-01, -3.8882e-01, -2.4535e-01,  2.4195e-01, -4.5718e-01,\n",
       "         -5.6169e-02,  5.5522e-02,  2.1639e-01, -1.6909e-01,  6.6829e-01,\n",
       "         -1.0079e-01,  4.8842e-01, -7.9480e-02, -8.2027e-01, -4.7512e-01,\n",
       "          3.0729e-01, -2.6621e-01,  6.2894e-01,  6.9960e-02,  2.0000e-01,\n",
       "          8.1740e-01,  5.6873e-01,  8.9102e-01, -5.3734e-01,  4.3909e-01,\n",
       "         -2.2533e-01, -4.2914e-01, -5.1384e-01,  9.0509e-02,  4.8844e-01,\n",
       "          2.5741e-01,  7.8796e-01, -3.7239e-01,  1.7399e-01,  6.1184e-01,\n",
       "         -3.8732e-01,  2.2962e-01, -3.8368e-02, -1.2208e-01, -8.4188e-01,\n",
       "          7.7880e-01,  6.8595e-01,  2.3518e-01, -1.9370e-02,  7.5326e-01,\n",
       "          2.0356e-01, -5.1931e-01,  1.1107e-01,  2.1855e-01, -6.7278e-01,\n",
       "         -1.0872e-01, -5.5397e-01,  2.1385e-01, -1.2451e-03, -1.7967e-02,\n",
       "          3.3863e-01,  8.8965e-01,  1.5200e-01,  2.7311e-01,  3.7653e-01,\n",
       "         -7.1380e-01,  1.6678e-01,  4.6528e-01, -4.2136e-01,  7.5002e-01,\n",
       "          2.9002e-01,  3.4358e-01, -3.5317e-01, -9.0789e-02, -5.3798e-01,\n",
       "         -1.0062e-01, -4.3579e-01, -1.1643e-01, -7.1553e-01, -2.7155e-01,\n",
       "         -7.0832e-01,  6.4915e-01, -7.7298e-01,  2.2276e-01, -7.7371e-01,\n",
       "         -7.9219e-02, -4.3705e-01, -5.4553e-01,  8.4737e-02,  2.1456e-01,\n",
       "          2.0207e-01,  4.3949e-01, -1.7827e-01,  6.1931e-01,  2.4582e-01,\n",
       "          9.9395e-02, -4.9635e-01,  4.5589e-01,  3.1786e-01, -1.4429e-01,\n",
       "          8.4958e-01,  5.3949e-01,  4.2001e-01,  8.8523e-02, -7.7147e-01,\n",
       "         -5.5243e-02, -9.5628e-02,  3.1315e-01, -3.4367e-02, -6.9749e-01,\n",
       "         -1.2654e-01,  2.6446e-01,  1.2145e-01,  7.4341e-02,  8.6312e-02,\n",
       "          4.8198e-02,  6.4779e-01, -6.9199e-01, -2.9954e-01, -4.8915e-01,\n",
       "         -1.5778e-01,  8.5216e-01,  2.6583e-01, -3.4117e-02, -7.5959e-02,\n",
       "          3.8816e-01, -4.7570e-01,  8.1654e-01, -4.5763e-02,  7.7568e-02,\n",
       "         -4.1905e-01,  6.2264e-01, -7.3777e-01,  6.2133e-02, -4.9801e-03,\n",
       "          4.0862e-01, -4.5146e-01,  3.8809e-01, -7.1952e-01, -5.6765e-01,\n",
       "          9.9291e-02, -3.8054e-01, -1.6740e-02,  3.3296e-02, -8.6955e-02,\n",
       "          4.0110e-01, -4.8812e-01, -4.3007e-01,  6.6260e-01, -4.8796e-01,\n",
       "         -1.9869e-01, -5.8544e-01, -2.3456e-01, -2.7236e-02,  2.5671e-02,\n",
       "          7.5429e-01,  3.7757e-01, -2.6806e-01,  1.5359e-01, -2.2283e-01,\n",
       "          5.9651e-01, -1.8393e-01,  6.7392e-01,  1.2476e-01,  7.8564e-01,\n",
       "          1.3999e-01,  7.4512e-01, -3.5467e-01]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37new",
   "language": "python",
   "name": "py37_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
